{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "import math \n",
    "import re\n",
    "import json \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(input):\n",
    "    if torch.cuda.is_available(): return input.cuda()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle(path+\"/data/step3_DAT_MT_USDJPY_M1_2018_merged_pickled\") \n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct():\n",
    "    pass \n",
    "\n",
    "def load_trumpdata(datapath, pad_tok='£', start_tok='^', end_tok='€'):\n",
    "\n",
    "    van_tws, tws, van_tw_str, tw_str = [],[],'',''\n",
    "    filenames = ['condensed_2018.json', 'condensed_2016.json', 'condensed_2017.json', 'condensed_2018.json']\n",
    "    for fname in filenames:\n",
    "        f = open(datapath+fname,\"r\")\n",
    "        data = f.readline()\n",
    "        f.close()\n",
    "        data_tr = json.loads(data)\n",
    "        for line in range(0,len(data_tr)):\n",
    "            tweet      = data_tr[line][\"text\"].rstrip('\\\\')\n",
    "            van_tw_str = van_tw_str + tweet \n",
    "            van_tws.append(tweet)            \n",
    "    symbols = list(set(van_tw_str))  \n",
    "    assert(pad_tok   not in symbols)\n",
    "    assert(start_tok not in symbols)\n",
    "    assert(end_tok   not in symbols)\n",
    "\n",
    "    for tweet in van_tws:\n",
    "        pad_tweet = start_tok + tweet + end_tok\n",
    "        tw_str    = tw_str + pad_tweet            \n",
    "        tws.append(pad_tweet)        \n",
    "    symbols = [pad_tok] + symbols + [start_tok, end_tok]   \n",
    "    decoder = {idx: symbols[idx] for idx in range(0,len(symbols))}\n",
    "    encoder = {symbols[idx]: idx for idx in range(0,len(symbols))}        \n",
    "    return tws, tw_str, decoder, encoder\n",
    "\n",
    "def pp_trumpdata(filename, prop, bsize=1):\n",
    "    Data, train, valid, test = Struct(), Struct(), Struct(), Struct()        \n",
    "    tweets, tweet_str, Data.decoder, Data.encoder = load_trumpdata(filename)    \n",
    "\n",
    "    train.tweets = tweets[0:round(prop[0]*len(tweets))]\n",
    "    train.tweet_str = tweet_str[0:round(prop[1]*len(tweet_str))]    \n",
    "    valid.tweets = tweets[round(prop[0]*len(tweets)):round(prop[1]*len(tweets))]\n",
    "    valid.tweet_str = tweet_str[round(prop[0]*len(tweet_str)):round(prop[1]*len(tweet_str))]    \n",
    "    test.tweets  = tweets[round(prop[1]*len(tweets)):-1]\n",
    "    test.tweet_str  = tweet_str[round(prop[1]*len(tweet_str)):-1]    \n",
    "\n",
    "    train.batch_str = []\n",
    "    stepsize = round(len(train.tweet_str)/bsize-1)\n",
    "    for i in range(0,bsize):\n",
    "        train.batch_str.append(train.tweet_str[i*stepsize:(i+1)*stepsize])\n",
    "    valid.batch_str = [valid.tweet_str]\n",
    "    \n",
    "    Data.train, Data.valid, Data.test, Data.bsize = train, valid, test, bsize\n",
    "    return Data\n",
    "\n",
    "def save_checkpoint(state, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename='models/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(filename)    \n",
    "    for item in iter(checkpoint):\n",
    "        print(item)\n",
    "    model = RNN(checkpoint['in_sz'],checkpoint['hd_sz'],checkpoint['out_sz'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, epoch, loss    \n",
    "\n",
    "def get_valid_loss(model,Data,Params,seq_len,ntweet):\n",
    "    criterion = nn.NLLLoss()\n",
    "    start = time.time()\n",
    "    loss_valid = 0\n",
    "    hidden = cuda(torch.zeros(1,model.hd_sz))\n",
    "    with torch.no_grad():    \n",
    "        model.eval()\n",
    "        for t in range(ntweet):\n",
    "            tweet = Data.valid.tweets[t]\n",
    "            xv, yv = generate_valid(Data,tweet,seq_len)     \n",
    "            loss = 0\n",
    "            for char in range(xv.size()[1]):\n",
    "                x = xv[:,char,:].reshape(xv.shape[0],xv.shape[2])\n",
    "                output, hidden = model.forward(x,hidden)\n",
    "                y = yv[:,char,:]\n",
    "                loss += criterion(output,y.reshape(xv.shape[0]))\n",
    "            loss_valid += loss/(xv.size()[2])\n",
    "#     print(f\"calculating validation loss took {time.time()-start:.2f} seconds\")\n",
    "    return loss_valid/ntweet\n",
    "\n",
    "def generate_valid(Data, tweet, seq_len):\n",
    "    if seq_len > len(tweet)-1: seq_len = len(tweet)-1    \n",
    "    X = torch.zeros(1,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(1,seq_len,1)  \n",
    "    x = encodestr(tweet[0:seq_len],Data.encoder)\n",
    "    y = torch.Tensor([Data.encoder[char] for char in tweet[1:seq_len+1]])\n",
    "    X[0,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "    Y[0,:,:] = y.reshape(seq_len,1)\n",
    "    return cuda(X),cuda(Y.long())\n",
    "\n",
    "def init_params(in_sz, bs, hd_sz=150):\n",
    "    Params = Struct()\n",
    "    Params.ni      = 3000\n",
    "    Params.ne      = 1\n",
    "    Params.hd_sz   = hd_sz\n",
    "    Params.in_sz   = in_sz\n",
    "    Params.sql     = 10\n",
    "    Params.iv_pr   = 200\n",
    "    Params.iv_pl   = 100\n",
    "    Params.n_e     = 1\n",
    "    Params.n_i     = 1000\n",
    "    Params.use_opt = True \n",
    "    Params.lr      = 0.0005\n",
    "    Params.bs      = bs\n",
    "    return Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder/decoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodestr(string, encoder):\n",
    "    x = torch.zeros((len(string),len(encoder)))\n",
    "    x[[idx for idx in range(0,len(string))],[encoder[char] for char in string]] = 1\n",
    "    return x\n",
    "\n",
    "def onehencode(symbol, encoder):\n",
    "    x = torch.zeros(len(encoder),1)\n",
    "    x[encoder[symbol]] = 1.0\n",
    "    return x.t()\n",
    "\n",
    "def encode(string, encoder):\n",
    "    return torch.Tensor([encoder[char] for char in y_str])\n",
    "\n",
    "def onehdecode(X,decoder):\n",
    "    string = ''\n",
    "    for char in range(X.shape[0]):\n",
    "        val, idx = torch.max(X[char],0)\n",
    "        string += decoder[idx.item()]\n",
    "    print(string)\n",
    "    \n",
    "def ydecode(Y,decoder):\n",
    "    string = ''\n",
    "    for char in range(Y.shape[0]): string += decoder[Y[char].item()]\n",
    "    print(string)\n",
    "\n",
    "def change_char(s, p, r):\n",
    "    return s[:p]+r+s[p+1:] \n",
    "\n",
    "def generate_seq(model,Data,sql,symbol='^'):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.initHidden(1)\n",
    "        result = symbol\n",
    "        for i in range(sql):\n",
    "            x = cuda(onehencode(symbol,Data.encoder))\n",
    "            output, hidden = model.forward(x,hidden)        \n",
    "            hidden = hidden.detach()\n",
    "            \n",
    "            prob     = np.exp(output[0].cpu().numpy())\n",
    "            cum_prob = np.cumsum(prob)\n",
    "            idx      = np.where(cum_prob - random.random() > 0)[0][0]\n",
    "            symbol   = Data.decoder[idx]\n",
    "            result  += symbol\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNoriginal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNoriginal,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        self.h1  = nn.Linear(input_size + hidden_size, hidden_size)               \n",
    "        self.o1   = nn.Linear(input_size + hidden_size, input_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)    \n",
    "        \n",
    "        hidden   = self.h1(combined)\n",
    "        hidden   = torch.tanh(hidden)\n",
    "        \n",
    "        output   = self.o1(combined)\n",
    "        output   = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNoriginal,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        combined = input_size+hidden_size\n",
    "        \n",
    "        self.h1      = nn.Linear(combined, hidden_size)               \n",
    "        self.o1      = nn.Linear(combined, combined)\n",
    "        self.bn1     = nn.BatchNorm1d(combined, combined)\n",
    "        self.relu    = nn.ReLU(combined,combined)\n",
    "\n",
    "        self.o2      = nn.Linear(combined, input_size)\n",
    "        self.bn2     = nn.BatchNorm1d(input_size,input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)    \n",
    "        \n",
    "        hidden   = self.h1(combined)\n",
    "        hidden   = torch.tanh(hidden)\n",
    "        \n",
    "        output   = self.o1(combined)\n",
    "        output   = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))\n",
    "\n",
    "\n",
    "    \n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders, Itterators, DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parentbatch(tweets, bs, sql, symbol='£'):\n",
    "    f\"\"\"each parent-batch will have different numbers of sub-batches depending on how long the tweets are\"\"\"\n",
    "    assert(len(tweets)/bs*10%2==0)\n",
    "    bch_strs = batch_strings(tweets,bs,sql)\n",
    "    parent_batches = []\n",
    "    for pb in range(len(bch_strs)):\n",
    "        bch       = bch_strs[pb]\n",
    "        n_tweet   = bs\n",
    "        n_segment = math.ceil(len(bch[0])/sql)\n",
    "        sbx = torch.zeros(n_tweet,n_segment,sql,len(Data.decoder))\n",
    "        sby = torch.zeros(n_tweet,n_segment,sql).long()\n",
    "\n",
    "        for tweet in range(n_tweet):\n",
    "            if re.search(symbol,bch[tweet]): position = re.search(symbol,bch[tweet]).span()[0]\n",
    "            else:                            position = len(bch[tweet])\n",
    "            x_str = change_char(bch[tweet],position-1,symbol)\n",
    "            y_str = bch[tweet][1:len(bch[tweet])]+symbol                \n",
    "            for segment in range(n_segment):\n",
    "                x = x_str[sql*segment:sql*(segment+1)]\n",
    "                y = y_str[sql*segment:sql*(segment+1)]  \n",
    "                sbx[tweet,segment] = encodestr(x,Data.encoder)\n",
    "                sby[tweet,segment] = torch.Tensor([Data.encoder[char] for char in y])                \n",
    "                \n",
    "        sb_ds = SBDataLoader(sbx, sby)\n",
    "        parent_batches.append(sb_ds)\n",
    "    return parent_batches\n",
    "\n",
    "\n",
    "def batch_strings(tweets,bs,sql=1):\n",
    "    f\"\"\"creates a list of batchsize-list of strings of same length and sort each batch with longest string first.\"\"\"\n",
    "    offset = -1*(int(len(tweets)/bs * 10) % 2 != 0)\n",
    "    bch_strs = [] \n",
    "    for i in range(round(len(tweets)/bs)+offset):\n",
    "        strings = tweets[i*bs:(i+1)*bs]\n",
    "        strings.sort(key=len,reverse=True)\n",
    "        pad_strings = pad(strings,sql)\n",
    "        bch_strs.append(pad_strings)\n",
    "    return bch_strs\n",
    "\n",
    "def pad(str_list,sql=1,token='£'):\n",
    "    f\"\"\"pad all strings in a list to max_len\"\"\"\n",
    "    max_len = math.ceil(len(max(str_list, key=len))/sql)*sql\n",
    "    for idx, row in enumerate(str_list):        \n",
    "        str_list[idx] = row + token*(max_len-len(row))\n",
    "    if len(str_list) == 1: return str_list[0]\n",
    "    return str_list\n",
    "\n",
    "class ParentDataLoader():\n",
    "    def __init__(self, ds): \n",
    "        self.ds = ds\n",
    "    def __iter__(self):    \n",
    "        for i in range(len(self.ds)):\n",
    "            iterator = iter(self.ds[i])\n",
    "            yield next(iterator), True\n",
    "            try:\n",
    "                while True:                \n",
    "                    yield next(iterator), False \n",
    "            except StopIteration:\n",
    "                pass\n",
    "\n",
    "class SBDataLoader():\n",
    "    def __init__(self, sbx, sby): \n",
    "        self.sbx, self.sby = sbx, sby\n",
    "    def __iter__(self):\n",
    "        for j in range(self.sbx.shape[1]): yield cuda(self.sbx[:,j]), cuda(self.sby[:,j])\n",
    "\n",
    "def train_model(model,Params,dataloader,n_itter,plot_valid=None):\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=Params.lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "    hidden = rnn.initHidden(Params.bs)    \n",
    "    \n",
    "    start = time.time()\n",
    "    if plot_valid is None: plot_valid = []\n",
    "    plot_train = []\n",
    "        \n",
    "    for i in range(n_itter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (X,Y), usezerostate    = next(dataloader)\n",
    "        X,Y                    = cuda(X),cuda(Y)\n",
    "        if usezerostate: hidden = rnn.initHidden(Params.bs)\n",
    "\n",
    "        loss = 0\n",
    "        for char in range(X.shape[1]):\n",
    "            x,y = X[:,char],Y[:,char]\n",
    "\n",
    "            f\"\"\"remove padded characters\"\"\"\n",
    "            idx = (y != 0).nonzero()\n",
    "            if idx.shape[0] == 0: break\n",
    "            if idx.shape[0] == 1: idx = idx.squeeze(0)\n",
    "            else:                 idx = idx.squeeze()\n",
    "\n",
    "            hidden = hidden[idx]\n",
    "            x = x[idx]\n",
    "            y = y[idx]\n",
    "\n",
    "\n",
    "            output,hidden = model.forward(x,hidden)\n",
    "            loss += criterion(output,y)        \n",
    "\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        if i%100==0: \n",
    "            plot_valid.append(get_valid_loss(rnn,Data,Params,30,50))\n",
    "        if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "    del optimizer\n",
    "    del criterion\n",
    "    print(f\"\"\"this training took {time.time()-start} seconds\"\"\")\n",
    "    return model,dataloader,plot_valid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs        = 15\n",
    "Data      = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "Params    = init_params(len(Data.encoder),bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = Data.train.tweets\n",
    "loader = ParentDataLoader(make_parentbatch(tweets,bs,sql=Params.sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: 0 itterations done in 0.6407127380371094 seconds\n",
      "checkpoint: 500 itterations done in 13.249198913574219 seconds\n",
      "checkpoint: 1000 itterations done in 25.665111780166626 seconds\n",
      "checkpoint: 1500 itterations done in 37.86642074584961 seconds\n",
      "checkpoint: 2000 itterations done in 50.09713387489319 seconds\n",
      "checkpoint: 2500 itterations done in 62.398521184921265 seconds\n",
      "this training took 73.95132422447205 seconds\n"
     ]
    }
   ],
   "source": [
    "# rnn = cuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "# dataloader = iter(copy.deepcopy(loader))\n",
    "n_itter = 3000\n",
    "rnn,dataloader,plot_valid = train_model(rnn,Params,dataloader,n_itter,plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(plot_valid[1:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: 0 itterations done in 0.7538394927978516 seconds\n",
      "this training took 12.662439584732056 seconds\n"
     ]
    }
   ],
   "source": [
    "temp_loader = iter(copy.deepcopy(dataloader))\n",
    "Plots = Struct()\n",
    "Plots.valid1 = [] \n",
    "\n",
    "start = time.time()\n",
    "for i in range(500):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    (X,Y), usezerostate    = next(temp_loader)\n",
    "    X,Y                    = cuda(X),cuda(Y)\n",
    "    if usezerostate: hidden = rnn.initHidden(bs)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for char in range(X.shape[1]):\n",
    "        x,y = X[:,char],Y[:,char]\n",
    "    \n",
    "        f\"\"\"remove padded characters\"\"\"\n",
    "        idx = (y != 0).nonzero()\n",
    "        if idx.shape[0] == 0: break\n",
    "        if idx.shape[0] == 1: idx = idx.squeeze(0)\n",
    "        else:                 idx = idx.squeeze()\n",
    "                                                \n",
    "        hidden = hidden[idx]\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        \n",
    "        output,hidden = rnn.forward(x,hidden)\n",
    "        loss += criterion(output,y)        \n",
    "\n",
    "    if loss != 0:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "    if i%100==0: \n",
    "        Plots.valid1.append(get_valid_loss(rnn,Data,Params,30,50))\n",
    "\n",
    "#         Plots.train_loss.append(loss/X.size()[2])\n",
    "    if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "        \n",
    "print(f\"\"\"this training took {time.time()-start} seconds\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model,Params,dataloader,n_itter,plot_valid=None)\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=Params.lr)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    start = time.time()\n",
    "    if plot_valid is None: plot_valid = []\n",
    "    plot_train = []\n",
    "        \n",
    "    for i in range(n_itter):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        (X,Y), usezerostate    = next(data_loader)\n",
    "        X,Y                    = cuda(X),cuda(Y)\n",
    "        if usezerostate: hidden = rnn.initHidden(Params.bs)\n",
    "\n",
    "        loss = 0\n",
    "        for char in range(X.shape[1]):\n",
    "            x,y = X[:,char],Y[:,char]\n",
    "\n",
    "            f\"\"\"remove padded characters\"\"\"\n",
    "            idx = (y != 0).nonzero()\n",
    "            if idx.shape[0] == 0: break\n",
    "            if idx.shape[0] == 1: idx = idx.squeeze(0)\n",
    "            else:                 idx = idx.squeeze()\n",
    "\n",
    "            hidden = hidden[idx]\n",
    "            x = x[idx]\n",
    "            y = y[idx]\n",
    "\n",
    "\n",
    "            output,hidden = model.forward(x,hidden)\n",
    "            loss += criterion(output,y)        \n",
    "\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        if i%100==0: \n",
    "            plot_valid.append(get_valid_loss(rnn,Data,Params,30,50))\n",
    "        if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "    del optimizer\n",
    "    del criterion\n",
    "    print(f\"\"\"this training took {time.time()-start} seconds\"\"\")\n",
    "    return model,dataloader,plot_valid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint: 0 itterations done in 0.6499161720275879 seconds\n",
      "checkpoint: 500 itterations done in 12.916450500488281 seconds\n",
      "checkpoint: 1000 itterations done in 24.76442527770996 seconds\n",
      "this training took 36.54662299156189 seconds\n"
     ]
    }
   ],
   "source": [
    "rnn2 = cuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "# rnn2.apply(weights_init_xavier)\n",
    "optimizer2 = optim.RMSprop(rnn2.parameters(), lr=Params.lr)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "temp_loader = iter(copy.deepcopy(dataloader))\n",
    "Plots.valid2 = [] \n",
    "\n",
    "start = time.time()\n",
    "for i in range(1500):\n",
    "    optimizer2.zero_grad()\n",
    "\n",
    "    (X,Y), usezerostate    = next(temp_loader)\n",
    "    X,Y                    = cuda(X),cuda(Y)\n",
    "    if usezerostate: hidden = rnn2.initHidden(bs)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for char in range(X.shape[1]):\n",
    "        x,y = X[:,char],Y[:,char]\n",
    "    \n",
    "        f\"\"\"remove padded characters\"\"\"\n",
    "        idx = (y != 0).nonzero()\n",
    "        if idx.shape[0] == 0: break\n",
    "        if idx.shape[0] == 1: idx = idx.squeeze(0)\n",
    "        else:                 idx = idx.squeeze()\n",
    "                                                \n",
    "        hidden = hidden[idx]\n",
    "        x = x[idx]\n",
    "        y = y[idx]\n",
    "\n",
    "        \n",
    "        output,hidden = rnn2.forward(x,hidden)\n",
    "        loss += criterion(output,y)        \n",
    "\n",
    "    if loss != 0:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "    if i%100==0: \n",
    "        Plots.valid2.append(get_valid_loss(rnn2,Data,Params,30,50))\n",
    "\n",
    "        Plots.train.append(loss/X.size()[2])\n",
    "    if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "        \n",
    "print(f\"\"\"this training took {time.time()-start} seconds\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "# plt.plot(Plots.valid1[1:-1])\n",
    "plt.plot(Plots.valid2[1:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate_seq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab888ad11ad7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate_seq' is not defined"
     ]
    }
   ],
   "source": [
    "generate_seq(rnn,Data,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'save_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a466553ec9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m save_checkpoint({\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             \u001b[0;34m'arch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"1_RNN\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;34m'state_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;34m'hd_sz'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhd_sz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'save_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': \"1_RNN\",\n",
    "            'state_dict': rnn.state_dict(),\n",
    "            'hd_sz': rnn.hd_sz,\n",
    "            'in_sz': rnn.in_sz,\n",
    "            'out_sz': rnn.out_sz,\n",
    "            'loss': loss,\n",
    "            'best_prec1': None,\n",
    "            'optimizer' : None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2, epoch, loss = load_checkpoint(filename='models/checkpoint.pth.tar')\n",
    "rnn2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_seq(rnn2, torch.zeros(1,hsize).cuda(),'T',100,m,42))\n",
    "print(generate_seq(rnn, torch.zeros(1,hsize).cuda(),'T',100,m,42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions I should not need anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_batch(model,X,Y,Data,hidden,lr,optimizer,use_opt,update_hidden):\n",
    "    model.train()\n",
    "    if use_opt: optimizer.zero_grad() \n",
    "    else: model.zero_grad()\n",
    "    loss = 0\n",
    "    for char in range(X.size()[1]):\n",
    "        x = X[:,char,:].reshape(X.shape[0],X.shape[2])\n",
    "        if update_hidden: hidden = parse_hidden(x,hidden,Data,symbol='*')        \n",
    "        output, hidden = model.forward(x,hidden)\n",
    "        y = Y[:,char,:]\n",
    "        loss += criterion(output,y.reshape(X.shape[0]))\n",
    "    loss.backward()\n",
    "    if use_opt: optimizer.step()\n",
    "    else:\n",
    "        for p in model.parameters(): p.data.add_(-lr, p.grad.data)\n",
    "    # hidden.detach() because we are done with training...\n",
    "    return loss/(X.size()[2]), hidden.detach()\n",
    "\n",
    "\n",
    "def generate_batch(e, Data, seq_len, get_valid=False):\n",
    "    if get_valid: \n",
    "        batch_str, bsize = Data.valid.batch_str, 1        \n",
    "    else: batch_str, bsize = Data.train.batch_str, Data.bsize\n",
    "    X = torch.zeros(bsize,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(bsize,seq_len,1)\n",
    "    for i in range(0,bsize):        \n",
    "        x = encodestr(batch_str[i][e:e+seq_len],Data.encoder)\n",
    "        y = torch.Tensor([Data.encoder[char] for char in batch_str[i][e+1:e+seq_len+1]])\n",
    "        X[i,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "        Y[i,:,:] = y.reshape(seq_len,1)\n",
    "    return cuda(X),cuda(Y.long())\n",
    "\n",
    "def do_training(model,Data,Params,optimizer,update_hidden,Plots=0):\n",
    "    if Plots==0:\n",
    "        Plots = Struct()\n",
    "        Plots.loss_train, Plots.loss_valid = [], []\n",
    "    start      = time.time()\n",
    "    loss_train = 0\n",
    "    hidden     = cuda(torch.zeros(Params.bsize,model.hd_sz))\n",
    "    for epoch in range(Params.ne):\n",
    "        char_idx = 0\n",
    "        i = 0 \n",
    "        while i < Params.ni and char_idx < len(Data.train.batch_str[0])-Params.sql-1:\n",
    "            X,Y          = generate_batch(char_idx, Data, Params.sql,False)\n",
    "            loss, hidden = train_batch(model,X,Y,Data,hidden,Params.lr,optimizer,True,update_hidden)\n",
    "            loss_train  += loss         \n",
    "            if i%Params.iv_pl  == 0:  \n",
    "                Plots.loss_valid.append(get_valid_loss(model,Data,Params,30,50))\n",
    "                print(Plots.loss_valid[-1])\n",
    "                Plots.loss_train.append(loss_train/Params.iv_pl)\n",
    "                loss_train = 0 \n",
    "            char_idx += Params.sql + 1\n",
    "            i        += 1\n",
    "        print(f\"\"\"\\n epoch {epoch+1} took {time.time() - start:.2f} seconds\"\"\")  \n",
    "    return Plots\n",
    "\n",
    "def parse_hidden(x,hidden,Data,symbol='*'):\n",
    "    # use .data to not break the connection to the graph     \n",
    "    for i in range(0,x.shape[0]):\n",
    "        if onehdecode(x[i,:],Data.decoder) == symbol:\n",
    "            hidden.data[i,:] = torch.zeros(1,hidden.shape[1])\n",
    "    return hidden\n",
    "\n",
    "class RNNoriginal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        self.h1  = nn.Linear(input_size + hidden_size, input_size + hidden_size)               \n",
    "        self.o1   = nn.Linear(input_size + hidden_size, input_size + hidden_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)    \n",
    "        \n",
    "        hidden   = self.h1(combined)\n",
    "        hidden   = torch.tanh(hidden)\n",
    "        \n",
    "        output   = self.o1(combined)\n",
    "        output   = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
