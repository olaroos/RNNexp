{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "import math \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchviz import make_dot\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usecuda(input):\n",
    "    if torch.cuda.is_available(): return input.cuda()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle(path+\"/data/step3_DAT_MT_USDJPY_M1_2018_merged_pickled\") \n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct():\n",
    "    pass \n",
    "\n",
    "def load_trumpdata(datapath):\n",
    "    import json \n",
    "    tweets, tweet_str = [], ''\n",
    "    filenames = ['condensed_2018.json', 'condensed_2016.json', 'condensed_2017.json', 'condensed_2018.json']\n",
    "    for fname in filenames:\n",
    "        f = open(datapath+fname,\"r\")\n",
    "        data = f.readline()\n",
    "        f.close()\n",
    "        data_tr = json.loads(data)\n",
    "        for line in range(0,len(data_tr)):\n",
    "            tweet = '*' + data_tr[line][\"text\"].rstrip('\\\\') + 'â‚¬'\n",
    "            tweets.append(tweet)\n",
    "            tweet_str = tweet_str + tweet            \n",
    "    symbols = list(set(tweet_str))\n",
    "    decoder = {idx: symbols[idx] for idx in range(0,len(symbols))}\n",
    "    encoder = {symbols[idx]: idx for idx in range(0,len(symbols))}        \n",
    "    return tweets, tweet_str, decoder, encoder\n",
    "\n",
    "def pp_trumpdata(filename, prop, bsize=1):\n",
    "    Data, train, valid, test = Struct(), Struct(), Struct(), Struct()        \n",
    "    tweets, tweet_str, Data.decoder, Data.encoder = load_trumpdata(filename)    \n",
    "\n",
    "    train.tweets = tweets[0:round(prop[0]*len(tweets))]\n",
    "    train.tweet_str = tweet_str[0:round(prop[1]*len(tweet_str))]    \n",
    "    valid.tweets = tweets[round(prop[0]*len(tweets)):round(prop[1]*len(tweets))]\n",
    "    valid.tweet_str = tweet_str[round(prop[0]*len(tweet_str)):round(prop[1]*len(tweet_str))]    \n",
    "    test.tweets  = tweets[round(prop[1]*len(tweets)):-1]\n",
    "    test.tweet_str  = tweet_str[round(prop[1]*len(tweet_str)):-1]    \n",
    "\n",
    "    train.batch_str = []\n",
    "    stepsize = round(len(train.tweet_str)/bsize-1)\n",
    "    for i in range(0,bsize):\n",
    "        train.batch_str.append(train.tweet_str[i*stepsize:(i+1)*stepsize])\n",
    "    valid.batch_str = [valid.tweet_str]\n",
    "    \n",
    "    Data.train, Data.valid, Data.test, Data.bsize = train, valid, test, bsize\n",
    "    return Data\n",
    "\n",
    "\n",
    "#     for i in range(0,len(tweets),bs):\n",
    "        \n",
    "\n",
    "\n",
    "def onehencode(symbol, encoder):\n",
    "    x = torch.zeros(len(encoder),1)\n",
    "    x[encoder[symbol]] = 1.0\n",
    "    return usecuda(x.t())\n",
    "\n",
    "def encodestr(string, encoder, seq_len):\n",
    "    x = torch.zeros((seq_len,len(encoder)))\n",
    "    x[[idx for idx in range(0,seq_len)],[encoder[char] for char in string]] = 1\n",
    "    return usecuda(x)\n",
    "\n",
    "def generate_seq(model, hidden, symbol, seq_len, m, seed):\n",
    "    with torch.no_grad():\n",
    "        result_str = symbol\n",
    "        for i in range(seq_len):\n",
    "            x = onehencode(symbol,encoder)\n",
    "            output, new_hidden = model.forward(x,hidden)\n",
    "        \n",
    "            hidden = new_hidden.detach()\n",
    "            prob = np.exp(output.detach().data.cpu().numpy())\n",
    "            cum_prob = np.cumsum(prob)\n",
    "\n",
    "            a = random.random()\n",
    "            idx = np.where(cum_prob - a > 0)[0][0]\n",
    "            symbol = decoder[idx]\n",
    "            result_str += symbol\n",
    "\n",
    "        return result_str\n",
    "    \n",
    "def save_checkpoint(state, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename='models/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(filename)    \n",
    "    for item in iter(checkpoint):\n",
    "        print(item)\n",
    "    model = RNN(checkpoint['in_sz'],checkpoint['hd_sz'],checkpoint['out_sz'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, epoch, loss    \n",
    "\n",
    "def get_valid_loss(model,Data,Params,seq_len,ntweet):\n",
    "    start = time.time()\n",
    "    loss_valid = 0\n",
    "    hidden = usecuda(torch.zeros(1,model.hd_sz))\n",
    "    with torch.no_grad():    \n",
    "        model.eval()\n",
    "        for t in range(ntweet):\n",
    "            tweet = Data.valid.tweets[t]\n",
    "            xv, yv = generate_valid(Data,tweet,seq_len)     \n",
    "            loss = 0\n",
    "            for char in range(xv.size()[1]):\n",
    "                x = xv[:,char,:].reshape(xv.shape[0],xv.shape[2])\n",
    "                output, hidden = model.forward(x,hidden)\n",
    "                y = yv[:,char,:]\n",
    "                loss += criterion(output,y.reshape(xv.shape[0]))\n",
    "            loss_valid += loss/(xv.size()[2])\n",
    "    print(f\"calculating validation loss took {time.time()-start:.2f} seconds\")\n",
    "    return loss_valid/ntweet\n",
    "\n",
    "def parse_hidden(x,hidden,Data,symbol='*'):\n",
    "    # use .data to not break the connection to the graph     \n",
    "    for i in range(0,x.shape[0]):\n",
    "        if onehdecode(x[i,:],Data.decoder) == symbol:\n",
    "            hidden.data[i,:] = torch.zeros(1,hidden.shape[1])\n",
    "    return hidden\n",
    "\n",
    "def train_batch(model,X,Y,Data,hidden,lr,optimizer,use_opt,update_hidden):\n",
    "    model.train()\n",
    "    if use_opt: optimizer.zero_grad() \n",
    "    else: model.zero_grad()\n",
    "    loss = 0\n",
    "    for char in range(X.size()[1]):\n",
    "        x = X[:,char,:].reshape(X.shape[0],X.shape[2])\n",
    "        if update_hidden: hidden = parse_hidden(x,hidden,Data,symbol='*')        \n",
    "        output, hidden = model.forward(x,hidden)\n",
    "        y = Y[:,char,:]\n",
    "        loss += criterion(output,y.reshape(X.shape[0]))\n",
    "    loss.backward()\n",
    "    if use_opt: optimizer.step()\n",
    "    else:\n",
    "        for p in model.parameters(): p.data.add_(-lr, p.grad.data)\n",
    "    # hidden.detach() because we are done with training...\n",
    "    return loss/(X.size()[2]), hidden.detach()\n",
    "\n",
    "def generate_valid(Data, tweet, seq_len):\n",
    "    if seq_len > len(tweet)-1: seq_len = len(tweet)-1    \n",
    "    X = torch.zeros(1,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(1,seq_len,1)  \n",
    "    x = encodestr(tweet[0:seq_len],Data.encoder,seq_len)\n",
    "    y = torch.Tensor([Data.encoder[char] for char in tweet[1:seq_len+1]])\n",
    "    X[0,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "    Y[0,:,:] = y.reshape(seq_len,1)\n",
    "    return usecuda(X),usecuda(Y.long())\n",
    "\n",
    "def generate_batch(e, Data, seq_len, get_valid=False):\n",
    "    if get_valid: \n",
    "        batch_str, bsize = Data.valid.batch_str, 1        \n",
    "    else: batch_str, bsize = Data.train.batch_str, Data.bsize\n",
    "    X = torch.zeros(bsize,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(bsize,seq_len,1)\n",
    "    for i in range(0,bsize):        \n",
    "        x = encodestr(batch_str[i][e:e+seq_len],Data.encoder,seq_len)\n",
    "        y = torch.Tensor([Data.encoder[char] for char in batch_str[i][e+1:e+seq_len+1]])\n",
    "        X[i,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "        Y[i,:,:] = y.reshape(seq_len,1)\n",
    "    return usecuda(X),usecuda(Y.long())\n",
    "\n",
    "def do_training(model,Data,Params,optimizer,update_hidden,Plots=0):\n",
    "    if Plots==0:\n",
    "        Plots = Struct()\n",
    "        Plots.loss_train, Plots.loss_valid = [], []\n",
    "    start      = time.time()\n",
    "    loss_train = 0\n",
    "    hidden     = usecuda(torch.zeros(Params.bsize,model.hd_sz))\n",
    "    for epoch in range(Params.ne):\n",
    "        char_idx = 0\n",
    "        i = 0 \n",
    "        while i < Params.ni and char_idx < len(Data.train.batch_str[0])-Params.seq_len-1:\n",
    "            X,Y          = generate_batch(char_idx, Data, Params.seq_len,False)\n",
    "            loss, hidden = train_batch(model,X,Y,Data,hidden,Params.lr,optimizer,True,update_hidden)\n",
    "            loss_train  += loss         \n",
    "            if i%Params.iv_pl  == 0:  \n",
    "                Plots.loss_valid.append(get_valid_loss(model,Data,Params,30,50))\n",
    "                print(Plots.loss_valid[-1])\n",
    "                Plots.loss_train.append(loss_train/Params.iv_pl)\n",
    "                loss_train = 0 \n",
    "            char_idx += Params.seq_len + 1\n",
    "            i        += 1\n",
    "        print(f\"\"\"\\n epoch {epoch+1} took {time.time() - start:.2f} seconds\"\"\")  \n",
    "    return Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        self.h1 = nn.Linear(input_size + hidden_size, hidden_size)        \n",
    "        self.o1 = nn.Linear(input_size + hidden_size, input_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.tanh(self.h1(combined))\n",
    "        output = self.o1(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(hidden_layers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "Params = Struct()\n",
    "Params.hd_sz   = 150\n",
    "Params.in_sz   = len(Data.encoder)\n",
    "Params.seq_len = 30\n",
    "Params.iv_pr   = 200\n",
    "Params.iv_pl   = 100\n",
    "Params.ne      = 1\n",
    "Params.ni      = 1000\n",
    "Params.use_opt = True \n",
    "Params.lr      = 0.0005\n",
    "Params.bsize   = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self, y_hat, y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(str_list,max_len=0,token='%'):\n",
    "    if max_len == 0: max_len = len((max(str_list, key=len)))\n",
    "    for idx, row in enumerate(str_list):        \n",
    "        str_list[idx] = row + token*(max_len-len(row))\n",
    "    if len(str_list) == 1: return str_list[0]\n",
    "    return str_list\n",
    "\n",
    "def batch_strings(tweets,bs):\n",
    "    bch_strs = [] \n",
    "    for i in range(round(len(tweets)/bs)-1):\n",
    "        bch_strs.append(pad(tweets[i*bs:(i+1)*bs]))\n",
    "    return bch_strs\n",
    "\n",
    "# if get_valid: \n",
    "#         batch_str, bsize = Data.valid.batch_str, 1        \n",
    "#     else: batch_str, bsize = Data.train.batch_str, Data.bsize\n",
    "#     X = torch.zeros(bsize,seq_len,len(Data.encoder))\n",
    "#     Y = torch.zeros(bsize,seq_len,1)\n",
    "#     for i in range(0,bsize):        \n",
    "#         x = encodestr(batch_str[i][e:e+seq_len],Data.encoder,seq_len)\n",
    "#         y = torch.Tensor([Data.encoder[char] for char in batch_str[i][e+1:e+seq_len+1]])\n",
    "#         X[i,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "#         Y[i,:,:] = y.reshape(seq_len,1)\n",
    "#     return usecuda(X),usecuda(Y.long())\n",
    "\n",
    "def process(bch_strs, sql):\n",
    "    f\"\"\"each parent-batch will have different numbers of sub-batches depending on how long the tweets are\"\"\"\n",
    "    parent_batches = []    \n",
    "    for pb in range(len(bch_strs)):\n",
    "        n_sb   = len(bch_strs[pb])\n",
    "        n_strs = math.ceil(len(bch_strs[pb][0])/sql)\n",
    "        sbx = torch.zeros(n_strs,n_sb,sql,len(Data.decoder))\n",
    "        sby = torch.zeros(n_strs,n_sb,sql,1)\n",
    "        for i in range(0,len(bch_strs[pb][0]),sql):         \n",
    "            if i+sql > len(bch_strs[pb][0])-1: endchar = len(bch_strs[pb][0])\n",
    "            else: endchar = i+sql                \n",
    "            for sb in range(len(bch_strs[pb])):\n",
    "                x_str, y_str = bch_strs[pb][sb][i:endchar-1], bch_strs[pb][sb][i+1:endchar]\n",
    "                if len(x_str) < sql: x_str, y_str = pad([x_str],sql),pad([y_str],sql)                                 \n",
    "                x = encodestr(x_str,Data.encoder,len(x_str))\n",
    "                y = torch.Tensor([Data.encoder[char] for char in y_str])\n",
    "                sbx[int(i/sql),sb], sby[int(i/sql),sb] = x,y.unsqueeze(1)\n",
    "        parent_batches.append((sbx,sby))\n",
    "    return parent_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "bch_str = batch_strings(Data.train.tweets,2)\n",
    "# print(len(bch_str[2]))\n",
    "test = bch_str[0:3]\n",
    "# print(len(test[0][0]))\n",
    "parent_batches = process(test,sql=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "torch.Size([24, 2, 10, 347])\n",
      "torch.Size([28, 2, 10, 347])\n",
      "torch.Size([24, 2, 10, 347])\n"
     ]
    }
   ],
   "source": [
    "print(len(parent_batches))\n",
    "print(parent_batches[0][0].shape)\n",
    "print(parent_batches[1][0].shape)\n",
    "print(parent_batches[2][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for pb in range(len(parent_batches)):\n",
    "    for sb in range(len(parent_batches[pb])):        \n",
    "        print(len(parent_batches[pb][sb][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  I have multiple problems, first I could make the padded token a zero, by doing that I could remove those values \n",
    "#  when calculating the loss. \n",
    "\n",
    "#  Also, there is a problem of encoding strings that are shorter than the sql. Then the \n",
    "#  positions that take up that null-space will be zeros, without a 1. These positions should be encoded to the \n",
    "#  padded-token as well. In the way I wrote the batches right now I cannot put a short tensor into the X[i] position\n",
    "\n",
    "# first solve the problem of training by removing the last elements from X and Y that are zero-vectors \n",
    "\n",
    "# problem again, one batch holds multiple pairs of X,Y. I have to come up with a new concept: \n",
    "# what if my batches has a parent batch. the Parent batch pb stands for the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 347])\n",
      "torch.Size([15, 2, 4, 347])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (347) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 347].  Tensor sizes: [4]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-260-9da0a41b46c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'hejhejhejhejhej'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-252-d8b01cfd3f66>\u001b[0m in \u001b[0;36mget_xy\u001b[0;34m(b_str, sql, bs)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mbatches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (347) must match the existing size (4) at non-singleton dimension 1.  Target sizes: [4, 347].  Tensor sizes: [4]"
     ]
    }
   ],
   "source": [
    "batches = get_xy(['hejhejhejhejhej','dududududududud'],4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([264., 261.,  46., 264.])\n",
      "tensor([261.,  46., 264., 261.])\n",
      "tensor([ 46., 264., 261.,  46.])\n",
      "tensor([264., 261.,  54.,  54.])\n"
     ]
    }
   ],
   "source": [
    "for subb in batches[0]:\n",
    "    print(subb[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_str = ['hejhejhejhejhej']\n",
    "b_str[0][12:110]\n",
    "# b_str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string_to_xy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-198-a5b6ffe7474f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstring_to_xy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'string_to_xy' is not defined"
     ]
    }
   ],
   "source": [
    "bch_strs = setup_batches(Data1.train.tweets,bs=3)\n",
    "# now they are the same length\n",
    "sql = 6\n",
    "\n",
    "print(len(ds[0]))\n",
    "x,y = string_to_xy(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Dataset():\n",
    "    def __init__(self, x, y): self.x,self.y = x,y\n",
    "    def __len__(self): return len(self.x)\n",
    "    def __getitem__(self, i): return self.x[i],self.y[i]\n",
    "\n",
    "def get_dls(train_ds, valid_ds, tbs, vbs, **kwargs):\n",
    "    return (DataLoader(train_ds, batch_size=tbs, **kwargs),\n",
    "            DataLoader(valid_ds, batch_size=vbs, **kwargs))\n",
    "\n",
    "\n",
    "class DataLoader():\n",
    "    def __init__(self, ds, bs): self.ds, self.bs = ds,bs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.5009, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.3001, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2965, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2922, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2873, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2848, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2778, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2730, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2690, device='cuda:0')\n",
      "calculating validation loss took 0.31 seconds\n",
      "tensor(0.2649, device='cuda:0')\n",
      "\n",
      " epoch 1 took 21.58 seconds\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(24)\n",
    "rnn1 = usecuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "torch.manual_seed(24)\n",
    "rnn2 = usecuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer1 = optim.RMSprop(rnn1.parameters(), lr=Params.lr)\n",
    "optimizer2 = optim.RMSprop(rnn2.parameters(), lr=Params.lr)\n",
    "\n",
    "Params1 = copy.deepcopy(Params)\n",
    "Params2 = copy.deepcopy(Params)\n",
    "Params1.bsize = 10\n",
    "Params2.bsize = 10\n",
    "\n",
    "Data1 = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], Params1.bsize)\n",
    "Data2 = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], Params2.bsize)\n",
    "\n",
    "# Plots1 = do_training(rnn1,Data1,Params1,optimizer1,True)\n",
    "Plots2 = do_training(rnn2,Data2,Params2,optimizer2,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.5019, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2999, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2962, device='cuda:0')\n",
    "calculating validation loss took 0.33 seconds\n",
    "tensor(0.2914, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2868, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2834, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2763, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2716, device='cuda:0')\n",
    "calculating validation loss took 0.32 seconds\n",
    "tensor(0.2678, device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Plots1.loss_valid[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Plots2.loss_valid[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = generate_batch(15, Data2, Params.seq_len)\n",
    "\n",
    "\n",
    "hidden = torch.ones(10,150).cuda()\n",
    "target = onehencode('t',Data2.encoder)\n",
    "\n",
    "def parse_hidden(x, hidden,Data,symbol='*'):\n",
    "    new_hidden = copy.deepcopy(hidden.detach())\n",
    "    for i in range(0,x.shape[0]):\n",
    "        if onehdecode(x[i,:],Data.decoder) == symbol:\n",
    "            new_hidden[i,:] = torch.zeros(1,hidden.shape[1])\n",
    "    return new_hidden\n",
    "\n",
    "new_hidden = parse_hidden(X[:,4,:],hidden,Data2, symbol='t')\n",
    "            \n",
    "            \n",
    "# new_hidden = copy.deepcopy(x[:,0,:].detach())\n",
    "# new_hidden = torch.zeros(10,347).cuda()\n",
    "# for i in range(0,Params2.bsize):\n",
    "#     val, idx = torch.max(x[i,0,:],0)\n",
    "#     print(Data2.decoder[idx.item()])\n",
    "#     if Data2.decoder[idx.item()] == 't':\n",
    "#         new_hidden[i,:] = torch.zeros(1,347).cuda()\n",
    "        \n",
    "\n",
    "#         new_hidden[idx] = x[idx,0,:]\n",
    "\n",
    "#     target = np.array(onehencode('t',Data2.encoder).cpu())\n",
    "# x2 = np.array(x.cpu())\n",
    "# torch.ones(10,347).cuda()\n",
    "\n",
    "# print(np.where(x2[:,0,:] == target, 0, 1))\n",
    "\n",
    "# np.array(target)\n",
    "\n",
    "# new_hidden = torch.where(x[0:5,0,:] == target, torch.zeros(1,347).cuda() ,torch.ones(1,347).cuda())\n",
    "# print(new_hidden.shape)\n",
    "# print(new_hidden[:,0:5])\n",
    "\n",
    "# new_hidden = torch.repeat(new_hidden[0,:])\n",
    "# new_hidden = torch.mul(hidden, new_hidden)\n",
    "\n",
    "for idx in range(0,len(new_hidden)):\n",
    "    print(onehdecode(x[idx,4,:],Data2.decoder))\n",
    "    print(str(new_hidden[idx,0:10])+'\\n------------')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehdecode(vector, decoder):\n",
    "    val, idx = torch.max(vector,0)\n",
    "    return decoder[idx.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = generate_batch(15, Data2, Params.seq_len)\n",
    "# print(torch.where(x[0][0][:] == onehencode('*',Data2.encoder)))\n",
    "# print(x[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': \"1_RNN\",\n",
    "            'state_dict': rnn.state_dict(),\n",
    "            'hd_sz': rnn.hd_sz,\n",
    "            'in_sz': rnn.in_sz,\n",
    "            'out_sz': rnn.out_sz,\n",
    "            'loss': loss,\n",
    "            'best_prec1': None,\n",
    "            'optimizer' : None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2, epoch, loss = load_checkpoint(filename='models/checkpoint.pth.tar')\n",
    "rnn2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_seq(rnn2, torch.zeros(1,hsize).cuda(),'T',100,m,42))\n",
    "print(generate_seq(rnn, torch.zeros(1,hsize).cuda(),'T',100,m,42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build RNN network with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### cut input into seq length bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Open','High','Low','Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.2\n",
    "set_train = data[0:round(len(data)*cutoff)]\n",
    "set_test  = data[round(len(data)*cutoff):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sz = set_train.shape[1]\n",
    "hidden_sz = 100\n",
    "output_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_sz, hidden_sz, output_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in set_train.iterrows():\n",
    "#     print(np.array(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "\n",
    "def train(X,Y)\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(X.size()[0]):\n",
    "        output, hidden = rnn(X[i], hidden)\n",
    "        \n",
    "    loss = criterion(output, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
