{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "import math \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# from torchviz import make_dot\n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(input):\n",
    "    if torch.cuda.is_available(): return input.cuda()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle(path+\"/data/step3_DAT_MT_USDJPY_M1_2018_merged_pickled\") \n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct():\n",
    "    pass \n",
    "\n",
    "def load_trumpdata(datapath, pad_tok='£', start_tok='^', end_tok='€'):\n",
    "    import json \n",
    "    van_tws, tws, van_tw_str, tw_str = [],[],'',''\n",
    "    filenames = ['condensed_2018.json', 'condensed_2016.json', 'condensed_2017.json', 'condensed_2018.json']\n",
    "    for fname in filenames:\n",
    "        f = open(datapath+fname,\"r\")\n",
    "        data = f.readline()\n",
    "        f.close()\n",
    "        data_tr = json.loads(data)\n",
    "        for line in range(0,len(data_tr)):\n",
    "            tweet      = data_tr[line][\"text\"].rstrip('\\\\')\n",
    "            van_tw_str = van_tw_str + tweet \n",
    "            van_tws.append(tweet)            \n",
    "    symbols = list(set(van_tw_str))  \n",
    "    assert(pad_tok   not in symbols)\n",
    "    assert(start_tok not in symbols)\n",
    "    assert(end_tok   not in symbols)\n",
    "\n",
    "    for tweet in van_tws:\n",
    "        pad_tweet = start_tok + tweet + end_tok\n",
    "        tw_str    = tw_str + pad_tweet            \n",
    "        tws.append(pad_tweet)        \n",
    "    symbols = [pad_tok] + symbols + [start_tok, end_tok]    \n",
    "    decoder = {idx: symbols[idx] for idx in range(0,len(symbols))}\n",
    "    encoder = {symbols[idx]: idx for idx in range(0,len(symbols))}        \n",
    "    return tws, tw_str, decoder, encoder\n",
    "\n",
    "def pp_trumpdata(filename, prop, bsize=1):\n",
    "    Data, train, valid, test = Struct(), Struct(), Struct(), Struct()        \n",
    "    tweets, tweet_str, Data.decoder, Data.encoder = load_trumpdata(filename)    \n",
    "\n",
    "    train.tweets = tweets[0:round(prop[0]*len(tweets))]\n",
    "    train.tweet_str = tweet_str[0:round(prop[1]*len(tweet_str))]    \n",
    "    valid.tweets = tweets[round(prop[0]*len(tweets)):round(prop[1]*len(tweets))]\n",
    "    valid.tweet_str = tweet_str[round(prop[0]*len(tweet_str)):round(prop[1]*len(tweet_str))]    \n",
    "    test.tweets  = tweets[round(prop[1]*len(tweets)):-1]\n",
    "    test.tweet_str  = tweet_str[round(prop[1]*len(tweet_str)):-1]    \n",
    "\n",
    "    train.batch_str = []\n",
    "    stepsize = round(len(train.tweet_str)/bsize-1)\n",
    "    for i in range(0,bsize):\n",
    "        train.batch_str.append(train.tweet_str[i*stepsize:(i+1)*stepsize])\n",
    "    valid.batch_str = [valid.tweet_str]\n",
    "    \n",
    "    Data.train, Data.valid, Data.test, Data.bsize = train, valid, test, bsize\n",
    "    return Data\n",
    "\n",
    "def onehencode(symbol, encoder):\n",
    "    x = torch.zeros(len(encoder),1)\n",
    "    x[encoder[symbol]] = 1.0\n",
    "    return usecuda(x.t())\n",
    "\n",
    "def encodestr(string, encoder):\n",
    "    x = torch.zeros((len(string),len(encoder)))\n",
    "    x[[idx for idx in range(0,len(string))],[encoder[char] for char in string]] = 1\n",
    "    return cuda(x)\n",
    "\n",
    "\n",
    "def encodeYstr(string, encoder):\n",
    "    return cuda(torch.Tensor([encoder[char] for char in y_str]))\n",
    "\n",
    "def generate_seq(model, hidden, symbol, seq_len, m, seed):\n",
    "    with torch.no_grad():\n",
    "        result_str = symbol\n",
    "        for i in range(seq_len):\n",
    "            x = onehencode(symbol,encoder)\n",
    "            output, new_hidden = model.forward(x,hidden)\n",
    "        \n",
    "            hidden = new_hidden.detach()\n",
    "            prob = np.exp(output.detach().data.cpu().numpy())\n",
    "            cum_prob = np.cumsum(prob)\n",
    "\n",
    "            a = random.random()\n",
    "            idx = np.where(cum_prob - a > 0)[0][0]\n",
    "            symbol = decoder[idx]\n",
    "            result_str += symbol\n",
    "\n",
    "        return result_str\n",
    "    \n",
    "def save_checkpoint(state, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename='models/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(filename)    \n",
    "    for item in iter(checkpoint):\n",
    "        print(item)\n",
    "    model = RNN(checkpoint['in_sz'],checkpoint['hd_sz'],checkpoint['out_sz'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, epoch, loss    \n",
    "\n",
    "def get_valid_loss(model,Data,Params,seq_len,ntweet):\n",
    "    start = time.time()\n",
    "    loss_valid = 0\n",
    "    hidden = usecuda(torch.zeros(1,model.hd_sz))\n",
    "    with torch.no_grad():    \n",
    "        model.eval()\n",
    "        for t in range(ntweet):\n",
    "            tweet = Data.valid.tweets[t]\n",
    "            xv, yv = generate_valid(Data,tweet,seq_len)     \n",
    "            loss = 0\n",
    "            for char in range(xv.size()[1]):\n",
    "                x = xv[:,char,:].reshape(xv.shape[0],xv.shape[2])\n",
    "                output, hidden = model.forward(x,hidden)\n",
    "                y = yv[:,char,:]\n",
    "                loss += criterion(output,y.reshape(xv.shape[0]))\n",
    "            loss_valid += loss/(xv.size()[2])\n",
    "    print(f\"calculating validation loss took {time.time()-start:.2f} seconds\")\n",
    "    return loss_valid/ntweet\n",
    "\n",
    "def parse_hidden(x,hidden,Data,symbol='*'):\n",
    "    # use .data to not break the connection to the graph     \n",
    "    for i in range(0,x.shape[0]):\n",
    "        if onehdecode(x[i,:],Data.decoder) == symbol:\n",
    "            hidden.data[i,:] = torch.zeros(1,hidden.shape[1])\n",
    "    return hidden\n",
    "\n",
    "def train_batch(model,X,Y,Data,hidden,lr,optimizer,use_opt,update_hidden):\n",
    "    model.train()\n",
    "    if use_opt: optimizer.zero_grad() \n",
    "    else: model.zero_grad()\n",
    "    loss = 0\n",
    "    for char in range(X.size()[1]):\n",
    "        x = X[:,char,:].reshape(X.shape[0],X.shape[2])\n",
    "        if update_hidden: hidden = parse_hidden(x,hidden,Data,symbol='*')        \n",
    "        output, hidden = model.forward(x,hidden)\n",
    "        y = Y[:,char,:]\n",
    "        loss += criterion(output,y.reshape(X.shape[0]))\n",
    "    loss.backward()\n",
    "    if use_opt: optimizer.step()\n",
    "    else:\n",
    "        for p in model.parameters(): p.data.add_(-lr, p.grad.data)\n",
    "    # hidden.detach() because we are done with training...\n",
    "    return loss/(X.size()[2]), hidden.detach()\n",
    "\n",
    "def generate_valid(Data, tweet, seq_len):\n",
    "    if seq_len > len(tweet)-1: seq_len = len(tweet)-1    \n",
    "    X = torch.zeros(1,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(1,seq_len,1)  \n",
    "    x = encodestr(tweet[0:seq_len],Data.encoder,seq_len)\n",
    "    y = torch.Tensor([Data.encoder[char] for char in tweet[1:seq_len+1]])\n",
    "    X[0,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "    Y[0,:,:] = y.reshape(seq_len,1)\n",
    "    return usecuda(X),usecuda(Y.long())\n",
    "\n",
    "def generate_batch(e, Data, seq_len, get_valid=False):\n",
    "    if get_valid: \n",
    "        batch_str, bsize = Data.valid.batch_str, 1        \n",
    "    else: batch_str, bsize = Data.train.batch_str, Data.bsize\n",
    "    X = torch.zeros(bsize,seq_len,len(Data.encoder))\n",
    "    Y = torch.zeros(bsize,seq_len,1)\n",
    "    for i in range(0,bsize):        \n",
    "        x = encodestr(batch_str[i][e:e+seq_len],Data.encoder,seq_len)\n",
    "        y = torch.Tensor([Data.encoder[char] for char in batch_str[i][e+1:e+seq_len+1]])\n",
    "        X[i,:,:] = x.reshape(seq_len,len(Data.encoder))\n",
    "        Y[i,:,:] = y.reshape(seq_len,1)\n",
    "    return usecuda(X),usecuda(Y.long())\n",
    "\n",
    "def do_training(model,Data,Params,optimizer,update_hidden,Plots=0):\n",
    "    if Plots==0:\n",
    "        Plots = Struct()\n",
    "        Plots.loss_train, Plots.loss_valid = [], []\n",
    "    start      = time.time()\n",
    "    loss_train = 0\n",
    "    hidden     = usecuda(torch.zeros(Params.bsize,model.hd_sz))\n",
    "    for epoch in range(Params.ne):\n",
    "        char_idx = 0\n",
    "        i = 0 \n",
    "        while i < Params.ni and char_idx < len(Data.train.batch_str[0])-Params.sql-1:\n",
    "            X,Y          = generate_batch(char_idx, Data, Params.sql,False)\n",
    "            print(X.shape)\n",
    "            loss, hidden = train_batch(model,X,Y,Data,hidden,Params.lr,optimizer,True,update_hidden)\n",
    "            loss_train  += loss         \n",
    "            if i%Params.iv_pl  == 0:  \n",
    "                Plots.loss_valid.append(get_valid_loss(model,Data,Params,30,50))\n",
    "                print(Plots.loss_valid[-1])\n",
    "                Plots.loss_train.append(loss_train/Params.iv_pl)\n",
    "                loss_train = 0 \n",
    "            char_idx += Params.sql + 1\n",
    "            i        += 1\n",
    "        print(f\"\"\"\\n epoch {epoch+1} took {time.time() - start:.2f} seconds\"\"\")  \n",
    "    return Plots\n",
    "\n",
    "def init_params(in_sz, bs, hd_sz=150):\n",
    "    Params = Struct()\n",
    "    Params.hd_sz   = hd_sz\n",
    "    Params.in_sz   = in_sz\n",
    "    Params.sql     = 30\n",
    "    Params.iv_pr   = 200\n",
    "    Params.iv_pl   = 100\n",
    "    Params.n_e     = 1\n",
    "    Params.n_i     = 1000\n",
    "    Params.use_opt = True \n",
    "    Params.lr      = 0.0005\n",
    "    Params.bsize   = bs\n",
    "    return Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        self.h1 = nn.Linear(input_size + hidden_size, hidden_size)        \n",
    "        self.o1 = nn.Linear(input_size + hidden_size, input_size)\n",
    "        \n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = torch.tanh(self.h1(combined))\n",
    "        output = self.o1(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return torch.zeros(bs,self.hd_sz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders, Itterators, DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TweetDataSet(tweets, bs, sql):\n",
    "    list_strs = batch_strings(tweets, bs)\n",
    "    return make_parentbatch(list_strs, sql)\n",
    "\n",
    "class ParentDataLoader():\n",
    "    def __init__(self, ds): self.ds = ds\n",
    "    def __iter__(self):\n",
    "        for i in range(len(self.ds)): yield self.ds[i]\n",
    "\n",
    "class SBItterator():\n",
    "    def __init__(self, sbx, sby): \n",
    "        self.sbx, self.sby = sbx, sby\n",
    "        self.size, self.n  = sbx.shape[0], 0\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self.n == self.size: return None, None\n",
    "        x,y = self.sbx[self.n], self.sby[self.n]\n",
    "        self.n += 1\n",
    "        return x,y\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def make_parentbatch(bch_strs, sql):\n",
    "    f\"\"\"each parent-batch will have different numbers of sub-batches depending on how long the tweets are\"\"\"\n",
    "    parent_batches = []\n",
    "    for pb in range(len(bch_strs)):\n",
    "        n_sb   = len(bch_strs[pb])\n",
    "        n_strs = math.ceil(len(bch_strs[pb][0])/sql)\n",
    "        sbx = torch.zeros(n_strs,n_sb,sql,len(Data.decoder))\n",
    "        sby = torch.zeros(n_strs,n_sb,sql)\n",
    "        for i in range(0,len(bch_strs[pb][0]),sql):         \n",
    "            if i+sql > len(bch_strs[pb][0])-1: endchar = len(bch_strs[pb][0])\n",
    "            else: endchar = i+sql                \n",
    "            for sb in range(n_sb):\n",
    "                x_str, y_str = bch_strs[pb][sb][i:endchar-1], bch_strs[pb][sb][i+1:endchar]\n",
    "                if len(x_str) < sql: x_str, y_str = pad([x_str],sql),pad([y_str],sql)                                 \n",
    "                x = encodestr(x_str,Data.encoder)\n",
    "                y = torch.Tensor([Data.encoder[char] for char in y_str])\n",
    "                sbx[int(i/sql),sb], sby[int(i/sql),sb] = x,y\n",
    "        sb_ds = SBItterator(sbx, sby)\n",
    "        parent_batches.append(sb_ds)\n",
    "    return parent_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_strings(tweets,bs,sql=1):\n",
    "    f\"\"\"creates a list of batchsize-list of strings of same length and sort each batch with longest string first.\"\"\"\n",
    "    offset = -1*(int(len(tweets)/bs * 10) % 2 != 0)\n",
    "    bch_strs = [] \n",
    "    for i in range(round(len(tweets)/bs)+offset):\n",
    "        strings = tweets[i*bs:(i+1)*bs]\n",
    "        strings.sort(key=len,reverse=True)\n",
    "        pad_strings = pad(strings,sql)\n",
    "        bch_strs.append(pad_strings)\n",
    "    return bch_strs\n",
    "\n",
    "def pad(str_list,sql=1,token='£'):\n",
    "    f\"\"\"pad all strings in a list to max_len\"\"\"\n",
    "    max_len = math.ceil(len(max(str_list, key=len))/sql)*sql\n",
    "    for idx, row in enumerate(str_list):        \n",
    "        str_list[idx] = row + token*(max_len-len(row))\n",
    "    if len(str_list) == 1: return str_list[0]\n",
    "    return str_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_char(s, p, r):\n",
    "    return s[:p]+r+s[p+1:]\n",
    "\n",
    "\n",
    "import re\n",
    "def make_parentbatch(bch_strs, sql, symbol='£'):\n",
    "    f\"\"\"each parent-batch will have different numbers of sub-batches depending on how long the tweets are\"\"\"\n",
    "    parent_batches = []\n",
    "    for pb in range(len(bch_strs)):\n",
    "        bch       = bch_strs[pb]\n",
    "        n_tweet   = len(bch)\n",
    "        n_segment = math.ceil(len(bch[0])/sql)\n",
    "\n",
    "        sbx = torch.zeros(n_tweet,n_segment,sql,len(Data.decoder))\n",
    "        sby = torch.zeros(n_tweet,n_segment,sql)        \n",
    "        for tweet in range(n_tweet):\n",
    "            y_str = bch[tweet][1:len(bch[tweet])]+symbol\n",
    "            if re.search(symbol,bch[tweet]): x_str = change_char(bch[tweet],re.search(symbol,bch[tweet]).span()[0]-1,symbol)\n",
    "            else: x_str = change_char(bch[tweet],len(bch[tweet])-1,symbol)\n",
    "            for segment in range(n_segment):\n",
    "                x = x_str[sql*segment:sql*(segment+1)]\n",
    "                y = y_str[sql*segment:sql*(segment+1)]\n",
    "                \n",
    "                x = encodestr(x,Data.encoder)\n",
    "                y = torch.Tensor([Data.encoder[char] for char in y])                \n",
    "                sbx[tweet,segment], sby[tweet,segment] = x,y\n",
    "        sb_ds = SBItterator(sbx, sby)\n",
    "        parent_batches.append(sb_ds)\n",
    "    return parent_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 826,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<__main__.SBItterator at 0x7fe3e0724e10>,\n",
       " <__main__.SBItterator at 0x7fe3e0d939b0>]"
      ]
     },
     "execution_count": 826,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = ['aaaaaaaaa','bbbbb','ccc','dddd','ee','ffff']\n",
    "tweets = ['abcdefghi','bbbbb','ccc','dddd','ee','ffff']\n",
    "sql = 3\n",
    "bch_strs = batch_strings(tweets,bs,sql=sql)\n",
    "make_parentbatch(bch_strs, sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 703,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 3 \n",
    "Data      = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "Params    = init_params(len(Data.encoder),bs)\n",
    "rnn       = usecuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "optimizer = optim.RMSprop(rnn.parameters(), lr=Params.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^HAPPY NEW YEAR! https://t.co/bHoPDPQ7G6€\n",
      "['^HAPPY NEW YEAR! https://t.co/bHoPDPQ7G6€££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££££', '^....Senator Schumer, more than a year longer than any other Administration in history. These are people who have been approved by committees and all others, yet Schumer continues to hold them back from serving their Country! Very Unfair!€££££££££££££££££££££££££££££££', '^Heads of countries are calling wanting to know why Senator Schumer is not approving their otherwise approved Ambassadors!? Likewise in Government lawyers and others are being delayed at a record pace! 360 great and hardworking people are waiting for approval from....€']\n"
     ]
    }
   ],
   "source": [
    "ds   = TweetDataSet(Data.train.tweets[0:100], bs, Params.sql)\n",
    "p_dl = iter(ParentDataLoader(ds))\n",
    "sb_it = iter(next(p_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-695-0d49093006f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sb_it = iter(next(p_dl))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msb_it\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# for i in range(y.shape[1]):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "# sb_it = iter(next(p_dl))\n",
    "x,y = next(sb_it)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "# for i in range(y.shape[1]):\n",
    "#     print(y[:,i])\n",
    "# while x is mnot None:\n",
    "#     x,y = next(sb_it)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-619-c00440a9a281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(p_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rnn1 = usecuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "torch.manual_seed(24)\n",
    "rnn2 = usecuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "optimizer1 = optim.RMSprop(rnn1.parameters(), lr=Params.lr)\n",
    "optimizer2 = optim.RMSprop(rnn2.parameters(), lr=Params.lr)\n",
    "\n",
    "Params1 = copy.deepcopy(Params)\n",
    "Params2 = copy.deepcopy(Params)\n",
    "Params1.bsize = 10\n",
    "Params2.bsize = 10\n",
    "\n",
    "Data1 = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], Params1.bsize)\n",
    "Data2 = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], Params2.bsize)\n",
    "\n",
    "# Plots1 = do_training(rnn1,Data1,Params1,optimizer1,True)\n",
    "Plots2 = do_training(rnn2,Data2,Params2,optimizer2,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Plots1.loss_valid[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(Plots2.loss_valid[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehdecode(vector, decoder):\n",
    "    val, idx = torch.max(vector,0)\n",
    "    return decoder[idx.item()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_checkpoint({\n",
    "            'epoch': epoch,\n",
    "            'arch': \"1_RNN\",\n",
    "            'state_dict': rnn.state_dict(),\n",
    "            'hd_sz': rnn.hd_sz,\n",
    "            'in_sz': rnn.in_sz,\n",
    "            'out_sz': rnn.out_sz,\n",
    "            'loss': loss,\n",
    "            'best_prec1': None,\n",
    "            'optimizer' : None,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn2, epoch, loss = load_checkpoint(filename='models/checkpoint.pth.tar')\n",
    "rnn2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_seq(rnn2, torch.zeros(1,hsize).cuda(),'T',100,m,42))\n",
    "print(generate_seq(rnn, torch.zeros(1,hsize).cuda(),'T',100,m,42))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### build RNN network with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### cut input into seq length bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['Open','High','Low','Close']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.2\n",
    "set_train = data[0:round(len(data)*cutoff)]\n",
    "set_test  = data[round(len(data)*cutoff):-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sz = set_train.shape[1]\n",
    "hidden_sz = 100\n",
    "output_sz = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_sz, hidden_sz, output_sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for index, row in set_train.iterrows():\n",
    "#     print(np.array(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.005\n",
    "\n",
    "def train(X,Y)\n",
    "    hidden = rnn.initHidden()\n",
    "    \n",
    "    rnn.zero_grad()\n",
    "    \n",
    "    for i in range(X.size()[0]):\n",
    "        output, hidden = rnn(X[i], hidden)\n",
    "        \n",
    "    loss = criterion(output, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
