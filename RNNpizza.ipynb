{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "import math \n",
    "import re\n",
    "import json \n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random \n",
    "import pandas as pd\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cuda(input):\n",
    "    if torch.cuda.is_available(): return input.cuda()\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_pickle(path+\"/data/step3_DAT_MT_USDJPY_M1_2018_merged_pickled\") \n",
    "# data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Struct():\n",
    "    pass \n",
    "\n",
    "def load_trumpdata(datapath, pad_tok='£', start_tok='^', end_tok='€'):\n",
    "\n",
    "    van_tws, tws, van_tw_str, tw_str = [],[],'',''\n",
    "    filenames = ['condensed_2018.json', 'condensed_2016.json', 'condensed_2017.json', 'condensed_2018.json']\n",
    "    for fname in filenames:\n",
    "        f = open(datapath+fname,\"r\")\n",
    "        data = f.readline()\n",
    "        f.close()\n",
    "        data_tr = json.loads(data)\n",
    "        for line in range(0,len(data_tr)):\n",
    "            tweet      = data_tr[line][\"text\"].rstrip('\\\\')\n",
    "            van_tw_str = van_tw_str + tweet \n",
    "            van_tws.append(tweet)            \n",
    "    symbols = list(set(van_tw_str))  \n",
    "    assert(pad_tok   not in symbols)\n",
    "    assert(start_tok not in symbols)\n",
    "    assert(end_tok   not in symbols)\n",
    "\n",
    "    for tweet in van_tws:\n",
    "        pad_tweet = start_tok + tweet + end_tok\n",
    "        tw_str    = tw_str + pad_tweet            \n",
    "        tws.append(pad_tweet)        \n",
    "    symbols = [pad_tok, start_tok] + symbols + [end_tok]   \n",
    "    decoder = {idx: symbols[idx] for idx in range(0,len(symbols))}\n",
    "    encoder = {symbols[idx]: idx for idx in range(0,len(symbols))}        \n",
    "    return tws, tw_str, decoder, encoder\n",
    "\n",
    "def pp_trumpdata(filename, prop, bsize=1):\n",
    "    Data, train, valid, test = Struct(), Struct(), Struct(), Struct()        \n",
    "    tweets, tweet_str, Data.decoder, Data.encoder = load_trumpdata(filename)    \n",
    "\n",
    "    train.tweets = tweets[0:round(prop[0]*len(tweets))]\n",
    "    train.tweet_str = tweet_str[0:round(prop[1]*len(tweet_str))]    \n",
    "    valid.tweets = tweets[round(prop[0]*len(tweets)):round(prop[1]*len(tweets))]\n",
    "    valid.tweet_str = tweet_str[round(prop[0]*len(tweet_str)):round(prop[1]*len(tweet_str))]    \n",
    "    test.tweets  = tweets[round(prop[1]*len(tweets)):-1]\n",
    "    test.tweet_str  = tweet_str[round(prop[1]*len(tweet_str)):-1]    \n",
    "\n",
    "    train.batch_str = []\n",
    "    stepsize = round(len(train.tweet_str)/bsize-1)\n",
    "    for i in range(0,bsize):\n",
    "        train.batch_str.append(train.tweet_str[i*stepsize:(i+1)*stepsize])\n",
    "    valid.batch_str = [valid.tweet_str]\n",
    "    \n",
    "    Data.train, Data.valid, Data.test, Data.bsize = train, valid, test, bsize\n",
    "    return Data\n",
    "\n",
    "def save_checkpoint(state, filename='models/checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(filename='models/checkpoint.pth.tar'):\n",
    "    checkpoint = torch.load(filename)    \n",
    "    for item in iter(checkpoint):\n",
    "        print(item)\n",
    "    model = RNN(checkpoint['in_sz'],checkpoint['hd_sz'],checkpoint['out_sz'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    #     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    loss = checkpoint['loss']\n",
    "    return model, epoch, loss    \n",
    "\n",
    "\n",
    "def init_params(in_sz, bs, hd_sz=150):\n",
    "    Params = Struct()\n",
    "    Params.ni      = 3000\n",
    "    Params.ne      = 1\n",
    "    Params.hd_sz   = hd_sz\n",
    "    Params.in_sz   = in_sz\n",
    "    Params.sql     = 10\n",
    "    Params.iv_pr   = 200\n",
    "    Params.iv_pl   = 100\n",
    "    Params.n_e     = 1\n",
    "    Params.n_i     = 1000\n",
    "    Params.use_opt = True \n",
    "    Params.lr      = 0.0005\n",
    "    Params.bs      = bs\n",
    "    return Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encoder/decoders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodestr(string, encoder):\n",
    "    x = torch.zeros((len(string),len(encoder)))\n",
    "    x[[idx for idx in range(0,len(string))],[encoder[char] for char in string]] = 1\n",
    "    return x\n",
    "\n",
    "def onehencode(symbol, encoder):\n",
    "    x = torch.zeros(len(encoder),1)\n",
    "    x[encoder[symbol]] = 1.0\n",
    "    return x.t()\n",
    "\n",
    "def encode(string, encoder):\n",
    "    return torch.Tensor([encoder[char] for char in y_str])\n",
    "\n",
    "def onehdecode(X,decoder):\n",
    "    assert(X.shape[-1] == len(decoder))\n",
    "    string = ''\n",
    "    for char in range(X.shape[0]):\n",
    "        val, idx = torch.max(X[char],0)\n",
    "        string += decoder[idx.item()]\n",
    "    print(string)\n",
    "    \n",
    "def ydecode(Y,decoder):\n",
    "    string = ''\n",
    "    for char in range(Y.shape[0]): string += decoder[Y[char].item()]\n",
    "    print(string)\n",
    "\n",
    "def change_char(s, p, r):\n",
    "    return s[:p]+r+s[p+1:] \n",
    "\n",
    "def generate_seq(model,Data,sql,symbol='^'):\n",
    "    with torch.no_grad():\n",
    "        hidden = model.initHidden(1)\n",
    "        result = symbol\n",
    "        for i in range(sql):\n",
    "            x = cuda(onehencode(symbol,Data.encoder))\n",
    "            output, hidden = model.forward(x,hidden)        \n",
    "            hidden = hidden.detach()\n",
    "            \n",
    "            prob     = np.exp(output[0].cpu().numpy())\n",
    "            cum_prob = np.cumsum(prob)\n",
    "            idx      = np.where(cum_prob - random.random() > 0)[0][0]\n",
    "            symbol   = Data.decoder[idx]\n",
    "            result  += symbol\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## my RNN module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNoriginal(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNNoriginal,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        self.h1  = nn.Linear(input_size + hidden_size, hidden_size)               \n",
    "        self.o1   = nn.Linear(input_size + hidden_size, input_size)\n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)    \n",
    "        \n",
    "        hidden   = self.h1(combined)\n",
    "        hidden   = torch.tanh(hidden)\n",
    "        \n",
    "        output   = self.o1(combined)\n",
    "        output   = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN,self).__init__()\n",
    "        self.hd_sz  = hidden_size\n",
    "        self.in_sz  = input_size\n",
    "        self.out_sz = output_size\n",
    "        \n",
    "        combined = input_size+hidden_size\n",
    "        \n",
    "        self.h1      = nn.Linear(combined, hidden_size)               \n",
    "\n",
    "        self.o1      = nn.Linear(combined, combined)\n",
    "        self.bn1     = nn.BatchNorm1d(combined)\n",
    "        self.relu    = nn.ReLU(combined)\n",
    "\n",
    "        self.o2      = nn.Linear(combined, input_size)\n",
    "        self.bn2     = nn.BatchNorm1d(input_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)    \n",
    "        \n",
    "        hidden   = self.h1(combined)\n",
    "        hidden   = torch.tanh(hidden)\n",
    "        \n",
    "        output   = self.o1(combined)\n",
    "        output   = self.bn1(output)\n",
    "        output   = self.relu(output)\n",
    "        \n",
    "        output   = self.o2(output)\n",
    "        output   = self.bn2(output)\n",
    "        output   = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self,bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))\n",
    "\n",
    "def weights_init_uniform(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        # apply a uniform distribution to the weights and a bias=0\n",
    "        m.weight.data.uniform_(0.0, 1.0)\n",
    "        m.bias.data.fill_(0)\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # for every Linear layer in a model..\n",
    "    if classname.find('Linear') != -1:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders, Itterators, DataSets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(str_list,sql=1,token='£'):\n",
    "    f\"\"\"pad all strings in a list to max_len\"\"\"\n",
    "    max_len = math.ceil(len(max(str_list, key=len))/sql)*sql\n",
    "    for idx, row in enumerate(str_list):        \n",
    "        str_list[idx] = row + token*(max_len-len(row))\n",
    "    if len(str_list) == 1: return str_list[0]\n",
    "    return str_list\n",
    "\n",
    "def mk_tweetbatch(tweets,encoder,bs,sql,symbol='£'):\n",
    "    assert(math.floor(len(tweets)/bs)==len(tweets)/bs)\n",
    "    bch       = batch_strings(tweets,bs,sql)[0]\n",
    "    assert(math.floor(len(bch[0])/sql)==len(bch[0])/sql)            \n",
    "    n_segment = int(len(bch[0])/sql)\n",
    "    sbx       = torch.zeros(bs,n_segment,sql,len(encoder))\n",
    "    sby       = torch.zeros(bs,n_segment,sql).long()\n",
    "    for tweet in range(bs):\n",
    "        \"\"\"for target we don't use first char, compensate with one padded char\"\"\"\n",
    "        y_str = bch[tweet][1:len(bch[tweet])]+symbol      \n",
    "        \n",
    "        chng_pos = len(bch[tweet])\n",
    "        \"\"\"if we find padded char, we know that tweet ended, remove last char of tweet\"\"\"        \n",
    "        if re.search(symbol,bch[tweet]): chng_pos = re.search(symbol,bch[tweet]).span()[0]       \n",
    "        x_str = change_char(bch[tweet],chng_pos-1,symbol)     \n",
    "        \n",
    "        for segment in range(n_segment):\n",
    "            x = x_str[sql*segment:sql*(segment+1)]\n",
    "            y = y_str[sql*segment:sql*(segment+1)]  \n",
    "            sbx[tweet,segment] = encodestr(x,encoder)\n",
    "            sby[tweet,segment] = torch.Tensor([encoder[char] for char in y])                    \n",
    "    return sbx,sby\n",
    "\n",
    "class TweetDataLoader():\n",
    "    def __init__(self,data,tweets,bs,sql,shuffle=False):    \n",
    "        assert(math.floor(len(tweets)/bs)==len(tweets)/bs)\n",
    "        self.tweets  = tweets\n",
    "        self.bs      = bs         \n",
    "        self.sql     = sql\n",
    "        self.encoder = data.encoder\n",
    "        self.decoder = data.decoder\n",
    "        self.i       = -1\n",
    "        self.shuffle = shuffle        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.i = -1\n",
    "        \n",
    "    def __iter__(self):  \n",
    "        while True:\n",
    "            self.i+=1\n",
    "            twt      = self.tweets[self.i*self.bs:(self.i+1)*self.bs]\n",
    "            sbx,sby  = mk_tweetbatch(twt,self.encoder,self.bs,self.sql)\n",
    "            sbloader = iter(SBDataLoader(sbx,sby))            \n",
    "            yield next(sbloader)\n",
    "            try:\n",
    "                while True:                \n",
    "                    yield next(sbloader) \n",
    "            except StopIteration:\n",
    "                pass            \n",
    "            if self.i==round(len(self.tweets)/self.bs)-1:\n",
    "                if self.shuffle: random.shuffle(self.tweets)\n",
    "                break\n",
    "\n",
    "class SBDataLoader():\n",
    "    def __init__(self, sbx, sby): \n",
    "        self.sbx, self.sby = sbx, sby\n",
    "    def __iter__(self):\n",
    "        for j in range(self.sbx.shape[1]): yield cuda(self.sbx[:,j]), cuda(self.sby[:,j])\n",
    "\n",
    "def batch_strings(tweets,bs,sql=1):\n",
    "    f\"\"\"creates a list of batchsize-list of strings of same length and sort each batch with longest string first.\"\"\"    \n",
    "    \"\"\"NOT SURE ABOUT THIS OFFSET, BUT THE PREVIOUS CODE ALWAYS MADE A 0\"\"\"\n",
    "    offset = -1*((len(tweets)/bs)*10%2!=0)    \n",
    "#     offset = -1*((math.floor(len(tweets)/bs)==len(tweets)/bs)==0)    \n",
    "    bch_strs = [] \n",
    "    for i in range(round(len(tweets)/bs)+offset):\n",
    "        strings = tweets[i*bs:(i+1)*bs]\n",
    "        strings.sort(key=len,reverse=True)\n",
    "        pad_strings = pad(strings,sql)\n",
    "        bch_strs.append(pad_strings)\n",
    "    return bch_strs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### building FastAI classes to be used with callbacks in future\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, loss_fn, data):\n",
    "        opt = optim.RMSprop(model.parameters(), lr=Params.lr)\n",
    "        assert(model is not None)\n",
    "        assert(loss_fn is not None)\n",
    "        assert(data is not None)\n",
    "        self.model,self.opt,self.loss_fn,self.data = model,opt,loss_fn,data\n",
    "        \n",
    "\n",
    "class Callback():\n",
    "    def begin_fit(self,learn):\n",
    "        self.learn = learn\n",
    "        return True\n",
    "    def after_fit(self): return True\n",
    "    def begin_epoch(self,epoch):\n",
    "        self.epoch=epoch\n",
    "        return True\n",
    "    def begin_validate(self): return True\n",
    "    def after_epoch(self): return True \n",
    "    def begin_batch(self,xb,yb):\n",
    "        self.xb,self.yb = xb,yb\n",
    "        return True\n",
    "    def after_loss(self,loss):\n",
    "        self.loss=loss\n",
    "        return True\n",
    "    def after_backward(self): return True\n",
    "    def after_step(self): return True\n",
    "\n",
    "class TrainEvalCallback(Callback):\n",
    "    def begin_fit(self):\n",
    "        self.run.n_epochs=0.\n",
    "        self.run.n_iter=0\n",
    "    \n",
    "    def after_batch(self):\n",
    "        if not self.in_train: return\n",
    "        self.run.n_epochs += 1./self.iters\n",
    "        self.run.n_iter   += 1\n",
    "        \n",
    "    def begin_epoch(self):\n",
    "        self.run.n_epochs=self.epoch\n",
    "        self.model.train()\n",
    "        self.run.in_train=True\n",
    "\n",
    "    def begin_validate(self):\n",
    "        self.model.eval()\n",
    "        self.run.in_train=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CallBack' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-6816291ebcfb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'CallBack' is not defined"
     ]
    }
   ],
   "source": [
    "cb =  CallBack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb = TrainEvalCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalcb.after_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_rnn(epoches, learn, data, valid_loss=[], cb=None):\n",
    "    start = time.time()\n",
    "    for e in range(epoches):\n",
    "        i = 0\n",
    "        for xb, yb in data.train_dl:      \n",
    "            if xb[0,0,1].item() == 1: hidden = learn.model.initHidden(xb.shape[0])                \n",
    "            learn, hidden, loss = one_rnn_batch(learn,xb,yb,hidden)           \n",
    "            if (i%100==0): valid_loss.append(get_valid(learn,data))\n",
    "            i += 1\n",
    "    return learn, hidden, valid_loss\n",
    "\n",
    "def one_rnn_batch(learn,xb,yb,hidden):\n",
    "    loss = 0 \n",
    "    learn.model.train()\n",
    "    for char in range(xb.shape[1]):\n",
    "        x,y = xb[:,char],yb[:,char]\n",
    "\n",
    "        idx    = zero_idx(y)\n",
    "        if idx is None: break\n",
    "\n",
    "        hidden = hidden[idx]\n",
    "        x      = x[idx]\n",
    "        y      = y[idx]\n",
    "        output,hidden = learn.model.forward(x,hidden)\n",
    "        loss += learn.loss_fn(output,y)                \n",
    "    if loss != 0:\n",
    "            \n",
    "        loss.backward()\n",
    "        learn.opt.step()\n",
    "        learn.opt.zero_grad()\n",
    "        hidden = hidden.detach()\n",
    "    return learn, hidden, loss\n",
    "\n",
    "def get_valid_loss(model,data,Params,seq_len,ntweet):\n",
    "    criterion = nn.NLLLoss()\n",
    "    start = time.time()\n",
    "    loss_valid = 0\n",
    "    hidden = cuda(torch.zeros(1,model.hd_sz))\n",
    "    with torch.no_grad():    \n",
    "        model.eval()\n",
    "        for t in range(ntweet):\n",
    "            tweet = data.valid.tweets[t]\n",
    "            xv, yv = generate_valid(data,tweet,seq_len)     \n",
    "            loss = 0\n",
    "            for char in range(xv.size()[1]):\n",
    "                x = xv[:,char,:].reshape(xv.shape[0],xv.shape[2])\n",
    "                output, hidden = model.forward(x,hidden)\n",
    "                y = yv[:,char,:]\n",
    "                loss += criterion(output,y.reshape(xv.shape[0]))\n",
    "            loss_valid += loss/(xv.size()[2])\n",
    "#     print(f\"calculating validation loss took {time.time()-start:.2f} seconds\")\n",
    "    del criterion\n",
    "    return loss_valid/ntweet\n",
    "\n",
    "def generate_valid(data, tweet, seq_len):\n",
    "    if seq_len > len(tweet)-1: seq_len = len(tweet)-1    \n",
    "    X = torch.zeros(1,seq_len,len(data.encoder))\n",
    "    Y = torch.zeros(1,seq_len,1)  \n",
    "    x = encodestr(tweet[0:seq_len],data.encoder)\n",
    "    y = torch.Tensor([data.encoder[char] for char in tweet[1:seq_len+1]])\n",
    "    X[0,:,:] = x.reshape(seq_len,len(data.encoder))\n",
    "    Y[0,:,:] = y.reshape(seq_len,1)\n",
    "    return cuda(X),cuda(Y.long())\n",
    "\n",
    "\n",
    "def get_valid(learn,data):\n",
    "    learn.model.eval()\n",
    "    tot_loss = 0 \n",
    "    nb_it    = 0\n",
    "    for xb,yb in data.valid_dl:        \n",
    "        if xb[0,0,1].item() == 1: hidden = learn.model.initHidden(xb.shape[0])            \n",
    "        learn, hidden, loss = one_rnn_batch(learn,xb,yb,hidden)  \n",
    "        tot_loss += loss/xb.size()[2]\n",
    "        nb_it    += 1\n",
    "    print(f\"\"\"\"getting validation\"\"\")\n",
    "    return tot_loss/nb_it\n",
    "\n",
    "def zero_idx(y):\n",
    "    idx = (y != 0).nonzero()\n",
    "    if idx.shape[0] < 2: return None\n",
    "    else: idx = idx.squeeze()\n",
    "    return idx\n",
    "\n",
    "def stats(i,valid_loss):\n",
    "    if i%100==0: \n",
    "        valid_loss.append(get_valid_loss(learn.model,data,Params,30,50))\n",
    "    if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "    i += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-ece558a1cb00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweetDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_dl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTweetDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mParams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-50-066bc8a40142>\u001b[0m in \u001b[0;36mfit_rnn\u001b[0;34m(epoches, learn, data, valid_loss, cb)\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitHidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_rnn_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_valid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-066bc8a40142>\u001b[0m in \u001b[0;36mget_valid\u001b[0;34m(learn, data)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mtot_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mnb_it\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtot_loss\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnb_it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mzero_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "bs = 15\n",
    "data       = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "Params     = init_params(len(data.encoder),bs)\n",
    "Params.sql = 30\n",
    "Params.bs  = 15\n",
    "rnn = cuda(RNN(Params.in_sz, Params.hd_sz, 1))\n",
    "learn = Learner(rnn,nn.NLLLoss(),data)\n",
    "data.train_dl = iter(TweetDataLoader(data,data.train.tweets[0:1500],Params.bs,Params.sql))\n",
    "data.valid_dl = iter(TweetDataLoader(data,data.valid.tweets[0:60],Params.bs,Params.sql))\n",
    "learn, hidden, valid_loss = fit_rnn(1, learn, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f52cc89f898>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(valid_loss[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(learn, epochs, callbacks):\n",
    "    cb_handler = CallbackHandler(callbacks)\n",
    "    cb_handler.on_train_begin(epochs, learn)\n",
    "    for epoch in range(epochs):\n",
    "        learn.model.train()\n",
    "        cb_nalder.on_epoch_begin(epoch)\n",
    "        for xb, yb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs         = 15\n",
    "data       = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "Params     = init_params(len(Data.encoder),bs)\n",
    "Params.sql = 30\n",
    "Params.bs  = 15\n",
    "data.train_dl = iter(TweetDataLoader(data.train.tweets[0:200],Params.bs,Params.sql))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs         = 15\n",
    "data       = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "Params     = init_params(len(Data.encoder),bs)\n",
    "Params.sql = 30\n",
    "Params.bs  = 15\n",
    "# dataloader = iter(TweetDataLoader(Data.train.tweets,Params.bs,Params.sql))\n",
    "data.train_dl = iter(TweetDataLoader(data.train.tweets,Params.bs,Params.sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_valid = []\n",
    "# hidden = None\n",
    "n_itter = 3000\n",
    "\n",
    "learner = Learner(cuda(RNN(Params.in_sz, Params.hd_sz, 1)),nn.NLLLoss(),data)\n",
    "learner,plot_valid = train_model(learner,Params,n_itter,plot_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_seq(rnn,Data,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(plot_valid[1:-1])\n",
    "plt.show()\n",
    "print(plot_valid[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "# plt.plot(Plots.valid1[1:-1])\n",
    "plt.plot(Plots.valid2[1:-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_parentbatch(tweets, bs, sql, symbol='£'):\n",
    "    f\"\"\"each parent-batch will have different numbers of sub-batches depending on how long the tweets are\"\"\"\n",
    "    assert(len(tweets)/bs*10%2==0)\n",
    "    bch_strs = batch_strings(tweets,bs,sql)\n",
    "    parent_batches = []\n",
    "    for pb in range(len(bch_strs)):\n",
    "        bch       = bch_strs[pb]\n",
    "        n_tweet   = bs\n",
    "        n_segment = math.ceil(len(bch[0])/sql)\n",
    "        sbx = torch.zeros(n_tweet,n_segment,sql,len(Data.decoder))\n",
    "        sby = torch.zeros(n_tweet,n_segment,sql).long()\n",
    "\n",
    "        for tweet in range(n_tweet):\n",
    "            if re.search(symbol,bch[tweet]): position = re.search(symbol,bch[tweet]).span()[0]\n",
    "            else:                            position = len(bch[tweet])\n",
    "            x_str = change_char(bch[tweet],position-1,symbol)\n",
    "            y_str = bch[tweet][1:len(bch[tweet])]+symbol                \n",
    "            for segment in range(n_segment):\n",
    "                x = x_str[sql*segment:sql*(segment+1)]\n",
    "                y = y_str[sql*segment:sql*(segment+1)]  \n",
    "                sbx[tweet,segment] = encodestr(x,Data.encoder)\n",
    "                sby[tweet,segment] = torch.Tensor([Data.encoder[char] for char in y])                \n",
    "                \n",
    "        sb_ds = SBDataLoader(sbx, sby)\n",
    "        parent_batches.append(sb_ds)\n",
    "    return parent_batches\n",
    "\n",
    "class ParentDataLoader():\n",
    "    def __init__(self, ds): \n",
    "        self.ds = ds\n",
    "    def __iter__(self):    \n",
    "        for i in range(len(self.ds)):\n",
    "            iterator = iter(self.ds[i])\n",
    "            yield next(iterator), True\n",
    "            try:\n",
    "                while True:                \n",
    "                    yield next(iterator), False \n",
    "            except StopIteration:\n",
    "                pass\n",
    "            \n",
    "def train_model(learner,Params,n_itter,plot_valid=None,hidden=None):\n",
    "#     optimizer = optim.Adam(model.parameters(), lr=Params.lr)\n",
    "    \n",
    "    start = time.time()\n",
    "    if learner.opt is None: learner.opt = optim.RMSprop(learner.model.parameters(), lr=Params.lr)\n",
    "    if plot_valid  is None: plot_valid  = []\n",
    "    if hidden      is None: hidden      = learner.model.initHidden(Params.bs)    \n",
    "        \n",
    "    for i in range(n_itter):\n",
    "        (X,Y), usezerostate     = next(data.train_dl)\n",
    "        if usezerostate: hidden = learner.model.initHidden(Params.bs)\n",
    "\n",
    "        loss = 0\n",
    "        for char in range(X.shape[1]):\n",
    "            x,y = X[:,char],Y[:,char]\n",
    "\n",
    "            idx = zero_idx(y)\n",
    "            if idx is None: break\n",
    "            hidden = hidden[idx]\n",
    "            x      = x[idx]\n",
    "            y      = y[idx]\n",
    "            \n",
    "            output,hidden = learner.model.forward(x,hidden)\n",
    "            loss += learner.loss_fn(output,y)\n",
    "        if loss != 0:\n",
    "            loss.backward()\n",
    "            learner.opt.step()\n",
    "            learner.opt.zero_grad()\n",
    "            hidden = hidden.detach()\n",
    "\n",
    "        if i%100==0: \n",
    "            plot_valid.append(get_valid_loss(learner.model,Data,Params,30,50))\n",
    "        if i%500==0: print(f\"\"\"checkpoint: {i} itterations done in {time.time() - start} seconds\"\"\")\n",
    "\n",
    "    print(f\"\"\"this training took {time.time()-start} seconds\"\"\")\n",
    "    return learner,plot_valid,hidden            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions I should not need anymore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
