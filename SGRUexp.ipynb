{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "from ola_cb import * \n",
    "from ola_RNN import * \n",
    "from ola_models import * \n",
    "\n",
    "import os, time, copy, math, re, json, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGRU(nn.Module):\n",
    "    def __init__(self, in_sz, hd_sz, n_stacks):\n",
    "        super(SGRU,self).__init__()\n",
    "        self.in_sz = in_sz\n",
    "        self.hd_sz = hd_sz\n",
    "        self.n_stacks = n_stacks\n",
    "        self.GRUs = []\n",
    "        for i in range(n_stacks):\n",
    "            self.GRUs.append(GRU(in_sz,hd_sz))\n",
    "                \n",
    "    def batch_forward(self,xb,yb,hd):\n",
    "        # xb,yb,hd should be tensors     \n",
    "        # I want preds and hidden outputs to have size [sql, n_stacks, _]\n",
    "        preds = torch.zeros(size(xb,1), self.n_stacks, self.in_sz)\n",
    "        hidds = torch.zeros(size(xb,1)+1, self.n_stacks, self.hd_sz)\n",
    "        hidds[0] = hd\n",
    "        for char in range(size(preds,1)):\n",
    "            gru = self.GRUs[char]            \n",
    "            preds[char],hidds[char+1] = gru.forward(xb[char],yb[char],hd[char])\n",
    "            for stack in range(1,self.n_stacks):\n",
    "                \n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_sz, hd_sz):\n",
    "        super(GRU,self).__init__()\n",
    "        self.in_sz = in_sz\n",
    "        self.hd_sz = hd_sz\n",
    "        \n",
    "        self.h_lin = nn.Linear(self.hd_sz,3*self.hd_sz)\n",
    "        self.x_lin = nn.Linear(self.in_sz,3*self.hd_sz)        \n",
    "        \n",
    "        self.up_sig = nn.Sigmoid()\n",
    "        self.re_sig = nn.Sigmoid()\n",
    "            \n",
    "        self.o1      = nn.Linear(self.hd_sz+self.in_sz,self.in_sz)  \n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)   \n",
    "        self.loss    = 0 \n",
    "            \n",
    "    def forward(self,input,hidden):        \n",
    "        x = self.x_lin(input)        \n",
    "        h = self.h_lin(hidden)        \n",
    "        x_u,x_r,x_n = x.chunk(3,1)\n",
    "        h_u,h_r,h_n = h.chunk(3,1)\n",
    "        update_gate = self.up_sig(x_u+h_u)        \n",
    "        reset_gate  = self.re_sig(x_r+h_r)\n",
    "        new_gate    = torch.tanh(x_n + reset_gate * h_n)         \n",
    "        h_new       = update_gate * hidden + (1 - update_gate) * new_gate \n",
    "        \n",
    "        combined   = torch.cat((input,h_new),1)\n",
    "        combined   = self.o1(combined)\n",
    "\n",
    "        prediction = self.softmax(combined)\n",
    "        \n",
    "        return prediction, h_new\n",
    "    \n",
    "    def batch_forward(self,xb,yb,hidden,loss_fn):\n",
    "        self.train()\n",
    "        if xb[0,0,1].item() == 1: hidden = self.initHidden(xb.shape[0])                   \n",
    "        loss = 0 \n",
    "        for char in range(xb.shape[1]):\n",
    "            x,y           = xb[:,char],yb[:,char]\n",
    "            x,y,hidden    = unpad(x,y,hidden)\n",
    "            if x.shape[0] == 0: break\n",
    "            output,hidden = self.forward(x,hidden)\n",
    "            loss += loss_fn(output,y)    \n",
    "        return output,hidden.detach(),loss/(char+1)\n",
    "\n",
    "    def initHidden(self, bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
