{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "# from ola_RNN import * \n",
    "# from ola_trump import *\n",
    "# from ola_dataloader import * \n",
    "\n",
    "import os, time, copy, math, re, json, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehencode(symbol, encoder):\n",
    "    x = torch.zeros(len(encoder),1)\n",
    "    x[encoder[symbol]] = 1.0\n",
    "    return x.t()\n",
    "\n",
    "def yencode(string, encoder):\n",
    "    return torch.Tensor([encoder[char] for char in y_str])\n",
    "\n",
    "def onehdecode(X,decoder):\n",
    "    assert(X.shape[-1] == len(decoder))\n",
    "    string = ''\n",
    "    for char in range(X.shape[0]):\n",
    "        val, idx = torch.max(X[char],0)\n",
    "        string += decoder[idx.item()]\n",
    "    print(string)\n",
    "    \n",
    "def ydecode(Y,decoder):\n",
    "    string = ''\n",
    "    for char in range(Y.shape[0]): string += decoder[Y[char].item()]\n",
    "    print(string)\n",
    "\n",
    "\n",
    "def cuda(input):\n",
    "    if torch.cuda.is_available(): return input.cuda()\n",
    "    return input\n",
    "\n",
    "def encodestr(string, encoder):\n",
    "    x = torch.zeros((len(string),len(encoder)))\n",
    "    x[[idx for idx in range(0,len(string))],[encoder[char] for char in string]] = 1\n",
    "    return x\n",
    "\n",
    "def change_char(s, p, r):\n",
    "    return s[:p]+r+s[p+1:] \n",
    "\n",
    "def pad(str_list,sql=1,token='£'):\n",
    "    f\"\"\"pad all strings in a list to max_len\"\"\"\n",
    "    max_len = math.ceil(len(max(str_list, key=len))/sql)*sql\n",
    "    for idx, row in enumerate(str_list):        \n",
    "        str_list[idx] = row + token*(max_len-len(row))\n",
    "    if len(str_list) == 1: return str_list[0]\n",
    "    return str_list\n",
    "\n",
    "def mk_tweetbatch(tweets,encoder,bs,sql,symbol='£'):\n",
    "    assert(math.floor(len(tweets)/bs)==len(tweets)/bs)\n",
    "    bch       = batch_strings(tweets,bs,sql)[0]\n",
    "    assert(math.floor(len(bch[0])/sql)==len(bch[0])/sql)            \n",
    "    n_segment = int(len(bch[0])/sql)\n",
    "    sbx       = torch.zeros(bs,n_segment,sql,len(encoder))\n",
    "    sby       = torch.zeros(bs,n_segment,sql).long()\n",
    "    for tweet in range(bs):\n",
    "        \"\"\"for target we don't use first char, compensate with one padded char\"\"\"\n",
    "        y_str = bch[tweet][1:len(bch[tweet])]+symbol      \n",
    "        \n",
    "        chng_pos = len(bch[tweet])\n",
    "        \"\"\"if we find padded char, we know that tweet ended, remove last char of tweet\"\"\"        \n",
    "        if re.search(symbol,bch[tweet]): chng_pos = re.search(symbol,bch[tweet]).span()[0]       \n",
    "        x_str = change_char(bch[tweet],chng_pos-1,symbol)     \n",
    "        \n",
    "        for segment in range(n_segment):\n",
    "            x = x_str[sql*segment:sql*(segment+1)]\n",
    "            y = y_str[sql*segment:sql*(segment+1)]  \n",
    "            sbx[tweet,segment] = encodestr(x,encoder)\n",
    "            sby[tweet,segment] = torch.Tensor([encoder[char] for char in y])     \n",
    "    return sbx,sby\n",
    "\n",
    "class TweetDataLoader():\n",
    "    def __init__(self,data,tweets,bs,sql,shuffle=False):    \n",
    "#         assert(math.floor(len(tweets)/bs)==len(tweets)/bs)\n",
    "        self.tweets  = tweets\n",
    "        self.bs      = bs         \n",
    "        self.sql     = sql\n",
    "        self.encoder = data.encoder\n",
    "        self.decoder = data.decoder\n",
    "        self.i       = -1\n",
    "        self.ii      = 0 \n",
    "        self.shuffle = shuffle        \n",
    "        \n",
    "    def reset(self):\n",
    "        if self.shuffle: random.shuffle(self.tweets)\n",
    "        self.i  = -1\n",
    "        self.ii = 0\n",
    "        \n",
    "    def nb_itters(self):\n",
    "        return self.ii \n",
    "    \n",
    "    def __iter__(self):  \n",
    "        self.reset()\n",
    "        while True:\n",
    "            self.i += 1\n",
    "            twt      = self.tweets[self.i*self.bs:(self.i+1)*self.bs]\n",
    "            sbx,sby  = mk_tweetbatch(twt,self.encoder,self.bs,self.sql)\n",
    "            sbloader = iter(SBDataLoader(sbx,sby))            \n",
    "            try:\n",
    "                while True:                \n",
    "                    self.ii+=1                    \n",
    "                    yield next(sbloader) \n",
    "            except StopIteration:\n",
    "                self.ii-=1\n",
    "                pass            \n",
    "            if self.i==round(len(self.tweets)/self.bs)-2: \n",
    "                break\n",
    "\n",
    "class SBDataLoader():\n",
    "    def __init__(self, sbx, sby): \n",
    "        self.sbx, self.sby = sbx, sby\n",
    "    def __iter__(self):\n",
    "        for j in range(self.sbx.shape[1]): yield cuda(self.sbx[:,j]), cuda(self.sby[:,j])\n",
    "\n",
    "def batch_strings(tweets,bs,sql=1):\n",
    "    f\"\"\"creates a list of batchsize-list of strings of same length and sort each batch with longest string first.\"\"\"    \n",
    "    \"\"\"NOT SURE ABOUT THIS OFFSET, BUT THE PREVIOUS CODE ALWAYS MADE A 0\"\"\"\n",
    "    offset = -1*((len(tweets)/bs)*10%2!=0)    \n",
    "#     offset = -1*((math.floor(len(tweets)/bs)==len(tweets)/bs)==0)    \n",
    "    bch_strs = [] \n",
    "    for i in range(round(len(tweets)/bs)+offset):\n",
    "        strings = tweets[i*bs:(i+1)*bs]\n",
    "        strings.sort(key=len,reverse=True)\n",
    "        pad_strings = pad(strings,sql)\n",
    "        bch_strs.append(pad_strings)\n",
    "    return bch_strs\n",
    "\n",
    "\n",
    "class Struct():\n",
    "    pass \n",
    "\n",
    "def load_trumpdata(datapath, pad_tok='£', start_tok='^', end_tok='€'):\n",
    "\n",
    "    van_tws, tws, van_tw_str, tw_str = [],[],'',''\n",
    "    filenames = ['condensed_2018.json', 'condensed_2016.json', 'condensed_2017.json', 'condensed_2015.json']\n",
    "    for fname in filenames:\n",
    "        f = open(datapath+fname,\"r\")\n",
    "        data = f.readline()\n",
    "        f.close()\n",
    "        data_tr = json.loads(data)\n",
    "        for line in range(0,len(data_tr)):\n",
    "            tweet      = data_tr[line][\"text\"].rstrip('\\\\')\n",
    "            van_tw_str = van_tw_str + tweet \n",
    "            van_tws.append(tweet)            \n",
    "    symbols = list(set(van_tw_str))  \n",
    "    assert(pad_tok   not in symbols)\n",
    "    assert(start_tok not in symbols)\n",
    "    assert(end_tok   not in symbols)\n",
    "\n",
    "    for tweet in van_tws:\n",
    "        pad_tweet = start_tok + tweet + end_tok\n",
    "        tw_str    = tw_str + pad_tweet            \n",
    "        tws.append(pad_tweet)        \n",
    "    symbols = [pad_tok, start_tok] + symbols + [end_tok]   \n",
    "    decoder = {idx: symbols[idx] for idx in range(0,len(symbols))}\n",
    "    encoder = {symbols[idx]: idx for idx in range(0,len(symbols))}        \n",
    "    return tws, tw_str, decoder, encoder\n",
    "\n",
    "def pp_trumpdata(filename, prop, bsize=1):\n",
    "    Data, train, valid, test = Struct(), Struct(), Struct(), Struct()        \n",
    "    tweets, tweet_str, Data.decoder, Data.encoder = load_trumpdata(filename)    \n",
    "    train.tweets = tweets[0:round(prop[0]*len(tweets))]\n",
    "    train.tweet_str = tweet_str[0:round(prop[1]*len(tweet_str))]    \n",
    "    valid.tweets = tweets[round(prop[0]*len(tweets)):round(prop[1]*len(tweets))]\n",
    "    valid.tweet_str = tweet_str[round(prop[0]*len(tweet_str)):round(prop[1]*len(tweet_str))]    \n",
    "    test.tweets  = tweets[round(prop[1]*len(tweets)):-1]\n",
    "    test.tweet_str  = tweet_str[round(prop[1]*len(tweet_str)):-1]    \n",
    "\n",
    "    train.batch_str = []\n",
    "    stepsize = round(len(train.tweet_str)/bsize-1)\n",
    "    for i in range(0,bsize):\n",
    "        train.batch_str.append(train.tweet_str[i*stepsize:(i+1)*stepsize])\n",
    "    valid.batch_str = [valid.tweet_str]\n",
    "    \n",
    "    Data.train, Data.valid, Data.test, Data.bsize = train, valid, test, bsize\n",
    "    return Data\n",
    "\n",
    "\n",
    "def rnn_forward(learn,hidden,xb,yb):\n",
    "    learn.model.train()\n",
    "    if xb[0,0,1].item() == 1: hidden = learn.model.initHidden(xb.shape[0])                   \n",
    "    loss = 0 \n",
    "    for char in range(xb.shape[1]):\n",
    "        x,y = xb[:,char],yb[:,char]\n",
    "        idx = (y != 0).nonzero()\n",
    "        if idx.nelement() == 0: return output, hidden.detach(), loss/(char+1)\n",
    "        x,y,hidden = unpad_rnn(x,y,hidden)\n",
    "        output,hidden = learn.model.forward(x,hidden)\n",
    "        loss += learn.loss_fn(output,y)                \n",
    "\n",
    "    return output,hidden.detach(),loss/(char+1)\n",
    "\n",
    "def unpad_rnn(x,y,hidden):\n",
    "    idx = (y != 0).nonzero()        \n",
    "    if idx.shape[0] == 1: idx = idx[0]\n",
    "    else: idx = idx.squeeze()\n",
    "    return x[idx],y[idx],hidden[idx]\n",
    "\n",
    "def get_valid_rnn(learn,itters=30):\n",
    "    print(f\"\"\"getting validation\"\"\")    \n",
    "    learn.model.eval()\n",
    "    tot_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        hidden = learn.model.initHidden(15)\n",
    "        for xb,yb in iter(learn.data.valid_dl): \n",
    "            output, hidden, loss = rnn_forward(learn,hidden,xb,yb)  \n",
    "            if loss != 0: tot_loss += loss.item()/xb.shape[0]\n",
    "            if learn.data.valid_dl.nb_itters() == itters: \n",
    "                return tot_loss/learn.data.valid_dl.nb_itters()\n",
    "        \n",
    "    return tot_loss/learn.data.valid_dl.nb_itters()\n",
    "\n",
    "\n",
    "def generate_seq(model,Data,sql,symbol='^'):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = model.initHidden(1)\n",
    "        result = symbol\n",
    "        for i in range(sql):\n",
    "            x = cuda(onehencode(symbol,Data.encoder))\n",
    "            output, hidden = model.forward(x,hidden)        \n",
    "            hidden = hidden.detach()\n",
    "            \n",
    "            prob     = np.exp(output[0].cpu().numpy())\n",
    "            cum_prob = np.cumsum(prob)\n",
    "            idx      = np.where(cum_prob - random.random() > 0)[0][0]\n",
    "            symbol   = Data.decoder[idx]\n",
    "            result  += symbol\n",
    "    model.train()\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs  = 20\n",
    "sql = 30\n",
    "data          = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "data.train_dl = TweetDataLoader(data,data.train.tweets,bs,sql,shuffle=True)\n",
    "data.valid_dl = TweetDataLoader(data,data.valid.tweets,bs,sql,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "got one\n"
     ]
    }
   ],
   "source": [
    "bs = 20\n",
    "sql = 30\n",
    "go = True\n",
    "while go:\n",
    "    random.shuffle(data.train.tweets)\n",
    "    tweets = data.train.tweets[0:bs]\n",
    "    (sbx,sby) = mk_tweetbatch(tweets,data.encoder,bs,sql,symbol='£')\n",
    "    y = sby[:,-1,0]\n",
    "    idx = (y != 0).nonzero()\n",
    "    if idx.nelement() == 0: \n",
    "        print(\"got one\")\n",
    "        go = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['^I like Michael Douglas!€', '^\"@KeepCalmBC:  @TheView is a pitiful shell of its former self! ALL NEW hosts are needed! Just cancel it &amp; put it &amp; us out of our misery!\"€', '^Just spoke to @JohnKasich to express condolences and prayers to all for the horrible shooting of two great police officers from @WestervillePD. This is a true tragedy!€', '^Why would a very low ratings radio talk show host like Hugh Hewitt be doing the next debate on @CNN. He is just a 3rd rate \"gotcha\" guy!€', '^Failed presidential candidate Lindsey Graham should respect me. I destroyed his run, brought him from 7% to 0% when he got out. Now nasty!€', '^South Carolina was so great last night. Will be back soon!€', '^Congratulations to @gohermie for winning the @ShellHouOpen. We are all proud of you @TNGCBedminster &amp; all @TrumpGolf clubs! Great going!€', '^Army Master Sgt. Charles H. McDaniel, 32, of Vernon, Indiana, and Army Pfc. William H. Jones, 19, of Nash County, North Carolina, are the first American remains from...€', '^Today on Earth Day, we celebrate our beautiful forests, lakes and land. We stand committed to preserving the natural beauty of our nation.€', '^Thank you for your continued support!\\n#MakeAmericaGreatAgain https://t.co/cykncrrSRE€', '^Great meeting with @SenateMajLdr Mitch McConnell and Republican leaders in D.C. #Trump2016 https://t.co/R0NuOrZISX€', '^\"@breadman28 @realDonaldTrump  make America great again. We\\'re running out of time!\"€', '^We need to be smart, vigilant and tough. We need the courts to give us back our rights. We need the Travel Ban as an extra level of safety!€', '^\"@laurencristmann: @realDonaldTrump @topcota1SG @PhxKen i can\\'t wait to cast my vote for Donald J TRUMP in MAY and NOVEMBER\"  Thanks.€', '^\"@james_artherton: @realDonaldTrump full respect from a British guy. I wish I could vote for you! Trump for president!\"€', '^New Reuters poll! Thank you!\\n#MakeAmericaGreatAgain #Trump2016 https://t.co/KDVZoUia4I€', '^\"@Buster842: @realDonaldTrump The thought of Obama makes me want to puke. He is a big disgrace to America and all it stands for.\"€', '^Will be interviewed on @GMA at 7:00 A.M. Big wins last night!€', '^The safety of American people is my absolute highest priority. Heed the directions of your State and Local Officials. Please be prepared, be careful and be SAFE! https://t.co/YP7ssITwW9 https://t.co/LZIUCgdPTH€', '^MAKE AMERICA GREAT AGAIN!€']\n",
      "25\n",
      "147\n",
      "169\n",
      "138\n",
      "140\n",
      "60\n",
      "142\n",
      "170\n",
      "140\n",
      "86\n",
      "116\n",
      "86\n",
      "141\n",
      "135\n",
      "121\n",
      "88\n",
      "131\n",
      "63\n",
      "211\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "print(tweets)\n",
    "for idx in range(len(tweets)):\n",
    "    print(len(tweets[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
