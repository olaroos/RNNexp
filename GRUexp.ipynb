{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device cuda:0\n",
      "/home/r2/Documents/RNNexp\n"
     ]
    }
   ],
   "source": [
    "from ola_cb import * \n",
    "from ola_RNN import * \n",
    "\n",
    "import os, time, copy, math, re, json, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch, torchvision\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "from functools import partial \n",
    "\n",
    "cuda_available = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if cuda_available else \"cpu\")\n",
    "print(f'''using device {device}''')\n",
    "\n",
    "path = !pwd\n",
    "path = path[0]\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_gru_batch(xb,yb,cb):\n",
    "    pred, cb.learn.hidden, loss = cb.learn.model.batch_forward(xb,yb,cb.learn.hidden,learn.loss_fn)\n",
    "    if not cb.after_loss(loss): return    \n",
    "    loss.backward()\n",
    "    if not cb.after_backward(): return \n",
    "    cb.learn.opt.step()\n",
    "    if not cb.after_step(): return\n",
    "    cb.learn.opt.zero_grad()\n",
    "\n",
    "def fit_gru(epoches, learn, cb=None, itters=math.inf):\n",
    "    hidden = learn.model.initHidden(learn.data.train_dl.bs)\n",
    "    if not cb.begin_fit(learn):           return \n",
    "    for epoch in range(epoches):\n",
    "        if not cb.begin_epoch(epoch):     return             \n",
    "        for xb, yb in iter(learn.data.train_dl):   \n",
    "            if not cb.begin_batch(xb,yb): return  \n",
    "            one_gru_batch(xb,yb,cb)\n",
    "            if not cb.begin_validate():   return     \n",
    "            if cb.do_stop():              break \n",
    "        if not cb.after_epoch():          return\n",
    "    if not cb.after_fit():                return \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Learner():\n",
    "    def __init__(self, model, loss_fn, opt, data, lr):\n",
    "        self.model, self.opt, self.loss_fn, self.data = model, opt, loss_fn, data\n",
    "        self._lr     = opt.param_groups[0]['lr']\n",
    "        self.hidden  = None    \n",
    "        self.stats   = Struct()\n",
    "        self.stats.valid_loss = []\n",
    "        self.stats.train_loss = []          \n",
    "        self.n_epochs = 0.\n",
    "        self.n_iters  = 0\n",
    "        \n",
    "    @property\n",
    "    def lr(self):\n",
    "        return self._lr\n",
    "    \n",
    "    @lr.setter\n",
    "    def lr(self,lr):\n",
    "        self._lr = lr\n",
    "        for param_group in self.opt.param_groups:\n",
    "            param_group['lr'] = lr        \n",
    "            \n",
    "    def one_batch(self, i, xb, yb):\n",
    "        try:\n",
    "            self.iter = i \n",
    "            self.xb,self.yb = xb,yb;                       self('begin_batch')\n",
    "            self.pred = self.model(self.xb);               self('after_pred')\n",
    "            self.loss = self.loss_fn(self.pred, self.yb);  self('after_loss')\n",
    "            if not self.in_train: return\n",
    "            self.loss.backwards();                         self('after_backward')\n",
    "            self.opt.step();                               self('after_step')\n",
    "            self.opt.zero_grad();\n",
    "        except CancelBatchException:                       self('after_cancel_ batch')\n",
    "        finally:                                           self('after_batch')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParamScheduler(Callback):\n",
    "    _order=5\n",
    "    def __init__(self, pname, sched_func): self.pname,self.sched_func = pname,sched_func\n",
    "\n",
    "    def begin_fit(self,learn):\n",
    "        super().begin_fit(learn)\n",
    "        return True        \n",
    "        \n",
    "    def set_param(self):\n",
    "        for pg in self.learn.opt.param_groups:\n",
    "            pg[self.pname] = self.sched_func(self.learn.n_epochs)\n",
    "        return True\n",
    "    \n",
    "    def begin_batch(self,xb,yb): \n",
    "        if self.learn.in_train: self.set_param()\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StatsCallback(Callback):\n",
    "    _order = 10\n",
    "    \n",
    "    def begin_fit(self,learn):\n",
    "        super().begin_fit(learn)\n",
    "        self.learn.stats.lrs = []\n",
    "        return True\n",
    "\n",
    "    def after_loss(self,loss):\n",
    "        self.learn.stats.train_loss.append(loss.detach().cpu())    \n",
    "        return True\n",
    "    \n",
    "    def after_step(self):\n",
    "        self.learn.stats.lrs.append(self.learn.opt.param_groups[-1]['lr'])\n",
    "        return True\n",
    "        \n",
    "    def begin_validate(self):\n",
    "        if self.learn.n_iters%100 == 0:\n",
    "            self.learn.in_train = False            \n",
    "            self.learn.stats.valid_loss.append(get_valid_rnn(self.learn,itters=30))\n",
    "            print(f\"\"\"finished: {self.learn.n_epochs}%\"\"\")\n",
    "        return True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self, in_sz, hd_sz):\n",
    "        super(GRU,self).__init__()\n",
    "        self.in_sz = in_sz\n",
    "        self.hd_sz = hd_sz\n",
    "        \n",
    "        self.h_lin = nn.Linear(self.hd_sz,3*self.hd_sz)\n",
    "        self.x_lin = nn.Linear(self.in_sz,3*self.hd_sz)        \n",
    "        \n",
    "        self.up_sig = nn.Sigmoid()\n",
    "        self.re_sig = nn.Sigmoid()\n",
    "            \n",
    "        self.o1      = nn.Linear(self.hd_sz+self.in_sz,self.in_sz)  \n",
    "\n",
    "        self.softmax = nn.LogSoftmax(dim=1)   \n",
    "        self.loss    = 0 \n",
    "            \n",
    "    def forward(self,input,hidden):        \n",
    "        x = self.x_lin(input)        \n",
    "        h = self.h_lin(hidden)        \n",
    "        x_u,x_r,x_n = x.chunk(3,1)\n",
    "        h_u,h_r,h_n = h.chunk(3,1)\n",
    "        update_gate = self.up_sig(x_u+h_u)        \n",
    "        reset_gate  = self.re_sig(x_r+h_r)\n",
    "        new_gate    = torch.tanh(x_n + reset_gate * h_n)         \n",
    "        h_new       = update_gate * hidden + (1 - update_gate) * new_gate \n",
    "        \n",
    "        combined   = torch.cat((input,h_new),1)\n",
    "        combined   = self.o1(combined)\n",
    "\n",
    "        prediction = self.softmax(combined)\n",
    "        \n",
    "        return prediction, h_new\n",
    "    \n",
    "    def batch_forward(self,xb,yb,hidden,loss_fn):\n",
    "        self.train()\n",
    "        if xb[0,0,1].item() == 1: hidden = self.initHidden(xb.shape[0])                   \n",
    "        loss = 0 \n",
    "        for char in range(xb.shape[1]):\n",
    "            x,y           = xb[:,char],yb[:,char]\n",
    "            x,y,hidden    = unpad(x,y,hidden)\n",
    "            if x.shape[0] == 0: break\n",
    "            output,hidden = self.forward(x,hidden)\n",
    "            loss += loss_fn(output,y)    \n",
    "        return output,hidden.detach(),loss/(char+1)\n",
    "\n",
    "    \n",
    "    def initHidden(self, bs):\n",
    "        return cuda(torch.zeros(bs,self.hd_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs  = 20\n",
    "sql = 30 \n",
    "lr  = 0.0005 \n",
    "\n",
    "sched = combine_scheds([0.15, 0.25, 0.2, 0.4], [sched_cos(0.0005, 0.0008), sched_cos(0.0008, 0.0005),sched_lin(0.0005,0.0005),sched_cos(0.0005,0.00005)]) \n",
    "data          = pp_trumpdata(path+\"/data/trump/\", [0.9,0.95], bs)\n",
    "data.train_dl = TweetDataLoader(data,data.train.tweets,bs,sql,shuffle=True)\n",
    "data.valid_dl = TweetDataLoader(data,data.valid.tweets,bs,sql,shuffle=False)\n",
    "\n",
    "model  = cuda(GRU(len(data.decoder), 150))\n",
    "opt    = optim.RMSprop(model.parameters(), lr)\n",
    "\n",
    "learn  = Learner(model,  nn.NLLLoss(), opt , data, lr=lr)\n",
    "# , , ParamScheduler('lr',sched)]) StatsCallback()\n",
    "cbs    = CallbackHandler([CounterCallback(),StatsCallback(), ParamScheduler('lr', sched)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n",
      "getting validation\n",
      "finished: 0.0%\n"
     ]
    }
   ],
   "source": [
    "fit_gru(1,learn,cbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([x for x in range(len(learn.stats.lrs))],learn.stats.lrs,label='learningrate')\n",
    "plt.legend()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([x for x in range(len(learn.stats.train_loss))],learn.stats.train_loss,label='training_loss')\n",
    "plt.legend()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot([x for x in range(len(learn.stats.valid_loss))],learn.stats.valid_loss,label='vloss')\n",
    "plt.legend()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
